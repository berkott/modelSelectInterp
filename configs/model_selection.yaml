data:
    train_samp_per_class: 10000
    d_d: 5
    N: 21
    data_alphas: 
        - 0.1
        - 0.5
    train_prop: 0.8

model:
    family: gpt2
    n_embd: 64
    n_layer: 3
    n_head: 2

training:
    batch_size: 32
    lr: 0.0003
    save_every_epochs: 500
    epochs: 1000

out_dir: models/

wandb:
    name: "model_selection"
    project: modelSelectInterp
    entity: berkott
    notes: