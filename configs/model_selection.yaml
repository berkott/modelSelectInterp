data:
    train_samp_per_class: 10000
    d_d: 5
    N: 21
    data_alphas: 
        - 0.1
        - 0.8
    train_prop: 0.8

model:
    family: gpt2
    n_embd: 64
    n_layer: 3
    n_head: 2

training:
    batch_size: 32
    lr: 0.0004
    save_every_epochs: 250
    epochs: 10000

out_dir: ./models

wandb:
    name: "model_selection"
    project: modelSelectInterp
    entity: chsanford4
    notes: