{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from data_gen import get_data, format_data\n",
    "from models import TransformerModel\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import time\n",
    "import einops\n",
    "\n",
    "with open(f\"configs/model_selection.yaml\", \"r\") as yaml_file:\n",
    "    args = Munch.fromYAML(yaml_file)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (_read_in): Linear(in_features=3, out_features=64, bias=True)\n",
       "  (_backbone): GPT2Model(\n",
       "    (wte): Embedding(50257, 64)\n",
       "    (wpe): Embedding(21, 64)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (_read_out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(\n",
    "    n_dims=len(args.data.data_alphas) + 1,\n",
    "    n_positions=args.data.N,\n",
    "    n_layer=args.model.n_layer,\n",
    "    n_head=args.model.n_head,\n",
    "    n_embd=args.model.n_embd\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"models/model_epoch1750_time16868839754.pth\", map_location=torch.device('cpu'))) # TODO: Remove map_location\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphas: torch.Size([2]), X: torch.Size([2, 21, 3]), y: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "data_dict = get_data(alphas=args.data.data_alphas, N=args.data.N, d_d=args.data.d_d, train_samp_per_class=2)#args.data.train_samp_per_class)\n",
    "\n",
    "alphas, X, y = format_data(data_dict, train_samples_per_alpha=int(2 / len(args.data.data_alphas)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_read_in', '_backbone.wte', '_backbone.wpe', '_backbone.h.0.ln_1', '_backbone.h.0.attn.c_attn', '_backbone.h.0.attn.c_proj', '_backbone.h.0.ln_2', '_backbone.h.0.mlp.c_fc', '_backbone.h.0.mlp.c_proj', '_backbone.h.1.ln_1', '_backbone.h.1.attn.c_attn', '_backbone.h.1.attn.c_proj', '_backbone.h.1.ln_2', '_backbone.h.1.mlp.c_fc', '_backbone.h.1.mlp.c_proj', '_backbone.h.2.ln_1', '_backbone.h.2.attn.c_attn', '_backbone.h.2.attn.c_proj', '_backbone.h.2.ln_2', '_backbone.h.2.mlp.c_fc', '_backbone.h.2.mlp.c_proj', '_backbone.ln_f', '_read_out'])\n"
     ]
    }
   ],
   "source": [
    "def get_flat_weights(model):\n",
    "    flat_weights = {}\n",
    "    for name, module in model.named_modules():\n",
    "        try:\n",
    "            flat_weights[name] = module.weight\n",
    "        except:\n",
    "            pass\n",
    "    return flat_weights\n",
    "\n",
    "unformatted_flat_weights = get_flat_weights(model)\n",
    "print(unformatted_flat_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed.W_E': Parameter containing:\n",
      "tensor([[-0.0043,  0.0468, -0.0046,  ...,  0.0046,  0.0077, -0.0073],\n",
      "        [ 0.0088, -0.0115,  0.0034,  ...,  0.0360, -0.0267, -0.0020],\n",
      "        [ 0.0029, -0.0516, -0.0153,  ...,  0.0073,  0.0196, -0.0176],\n",
      "        ...,\n",
      "        [-0.0033,  0.0085, -0.0081,  ..., -0.0072,  0.0006, -0.0175],\n",
      "        [-0.0073,  0.0005,  0.0020,  ...,  0.0054, -0.0029, -0.0345],\n",
      "        [ 0.0275,  0.0208, -0.0232,  ..., -0.0212, -0.0038, -0.0148]],\n",
      "       requires_grad=True), 'pos_embed.W_pos': Parameter containing:\n",
      "tensor([[ 2.4808e-05, -1.2077e-01, -2.1305e-01,  ...,  4.8523e-01,\n",
      "         -2.6729e-01, -1.0821e+00],\n",
      "        [-8.6075e-02, -3.4190e-02, -1.0396e-02,  ...,  5.0130e-01,\n",
      "         -3.8300e-02, -5.6612e-01],\n",
      "        [-7.0768e-02,  9.7565e-02, -6.6980e-02,  ...,  4.5082e-01,\n",
      "         -1.9241e-01, -3.2436e-01],\n",
      "        ...,\n",
      "        [-4.5040e-02,  9.2468e-02, -2.5057e-02,  ..., -1.5930e-01,\n",
      "         -2.4148e-01, -2.8253e-01],\n",
      "        [-1.1311e-01,  6.1901e-02, -1.3372e-02,  ...,  9.0406e-02,\n",
      "         -1.5346e-01, -3.6578e-01],\n",
      "        [ 2.9671e-01, -4.1253e-01,  1.3209e-02,  ..., -6.3102e-01,\n",
      "          1.2576e-01, -4.6640e-01]], requires_grad=True), 'blocks.0.ln1.w': Parameter containing:\n",
      "tensor([ 4.6181e-01, -4.4795e-03,  2.3191e-01, -5.5465e-03,  1.3582e-01,\n",
      "         1.4030e-03,  1.3000e-01, -4.6791e-03,  2.1519e-02, -7.5088e-03,\n",
      "         1.2936e-01,  9.1494e-01,  1.4837e+00,  9.5484e-01,  7.4666e-01,\n",
      "         1.2213e-01,  1.5022e-01, -1.0265e-02, -5.9972e-03,  3.6388e-01,\n",
      "         6.9466e-01,  3.0644e-01,  2.0535e-03, -1.2719e-03,  1.5140e-01,\n",
      "         6.3607e-02,  3.4514e-01, -4.6895e-03,  3.6886e-02,  1.5088e-02,\n",
      "         9.5291e-01, -4.7226e-03,  3.3458e-01,  2.5677e-01,  1.3721e-01,\n",
      "         7.5492e-03,  5.9420e-01,  5.0573e-03,  2.9564e-01, -1.3186e-03,\n",
      "        -4.8374e-03,  3.4053e-01,  2.1622e-01,  3.9750e-01,  4.8236e-01,\n",
      "         2.9639e-02,  2.0940e+00,  5.2977e-01,  2.3454e-03, -7.2291e-03,\n",
      "        -3.2180e-03,  1.3617e+00, -3.3049e-03,  3.8456e-01, -4.0385e-01,\n",
      "        -3.3683e-03,  7.8787e-03,  2.1613e-01, -1.0300e-02, -1.0158e-02,\n",
      "        -3.4468e-03,  2.6571e-01,  1.6123e-01,  4.5786e-04],\n",
      "       requires_grad=True), 'blocks.0.ln1.b': Parameter containing:\n",
      "tensor([-2.8876e-01, -5.3686e-03,  1.2407e-01,  1.6120e-03,  5.2879e-02,\n",
      "        -7.5867e-03, -2.6116e-01, -4.1025e-03,  2.0863e-02, -5.8750e-03,\n",
      "        -1.5026e-01, -1.8333e-01, -5.9194e-03,  1.7816e-01,  1.9757e-01,\n",
      "        -7.5869e-02,  1.3381e-01, -5.1805e-03,  2.2864e-03,  1.0147e-01,\n",
      "         2.4075e-01, -2.1369e-01, -2.7030e-03,  2.8590e-03,  4.3861e-02,\n",
      "        -2.3775e-02,  7.8589e-03, -1.3284e-03, -1.2157e-02,  2.1143e-02,\n",
      "        -2.7669e-01,  2.3899e-03,  9.0726e-02, -8.1866e-02,  7.5341e-02,\n",
      "         1.9075e-03,  9.8182e-02, -3.1880e-03,  3.4791e-01, -4.5562e-03,\n",
      "         1.8610e-03, -2.2657e-02,  8.0586e-02, -4.3695e-02, -6.5551e-04,\n",
      "         2.9971e-02,  5.5555e-02, -3.5136e-01, -3.9426e-03, -2.8807e-03,\n",
      "        -5.8790e-03,  3.3105e-02,  2.4958e-03,  5.3272e-02, -6.3247e-02,\n",
      "        -3.0868e-04,  1.2496e-03, -2.4283e-01, -1.7175e-03,  6.5620e-03,\n",
      "        -4.1091e-03,  1.1406e-01,  6.4996e-03, -5.0478e-03],\n",
      "       requires_grad=True), 'blocks.0.attn.W_Q': tensor([[[ 0.5823, -0.0503, -0.0400,  ...,  0.2478,  0.2495, -0.1601],\n",
      "         [-0.0740, -0.0428,  0.0373,  ...,  0.0259, -0.0239, -0.0691],\n",
      "         [-0.1702,  0.0120, -0.0318,  ..., -0.1174, -0.1804, -0.0134],\n",
      "         ...,\n",
      "         [-0.2251, -0.1503, -0.0518,  ...,  0.1597,  0.1810, -0.1435],\n",
      "         [-0.4359,  0.0929, -0.1389,  ..., -0.0804, -0.0573,  0.1528],\n",
      "         [-0.1640,  0.0892, -0.0858,  ..., -0.0672, -0.0174,  0.0549]],\n",
      "\n",
      "        [[-0.1991,  0.2452, -0.3857,  ...,  0.2520, -0.3777,  0.1318],\n",
      "         [ 0.0048, -0.0104, -0.0194,  ...,  0.0380,  0.0253, -0.0873],\n",
      "         [-0.1055,  0.0790, -0.0943,  ...,  0.0617, -0.1850,  0.0238],\n",
      "         ...,\n",
      "         [ 0.1205, -0.1141,  0.0978,  ..., -0.1254,  0.1320, -0.1515],\n",
      "         [-0.0369, -0.1246,  0.1083,  ..., -0.1128,  0.0564,  0.0262],\n",
      "         [ 0.0267, -0.0638,  0.0653,  ..., -0.0177, -0.1407, -0.0150]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.0.attn.W_K': tensor([[[ 0.3284, -0.0333,  0.0362,  ...,  0.5512,  0.5204, -0.0649],\n",
      "         [-0.1068,  0.0603,  0.0338,  ..., -0.0467, -0.0881,  0.0227],\n",
      "         [ 0.0063, -0.0183,  0.2906,  ...,  0.1803,  0.1795, -0.0392],\n",
      "         ...,\n",
      "         [-1.0389, -0.1173, -0.4185,  ...,  0.2455,  0.1790,  0.0363],\n",
      "         [-0.1732,  0.1926, -0.1745,  ..., -0.3417, -0.2747,  0.1240],\n",
      "         [ 0.0365,  0.0302,  0.2162,  ..., -0.0695, -0.0376,  0.0240]],\n",
      "\n",
      "        [[ 0.3290, -0.6670,  0.6191,  ..., -0.4204, -0.0340, -0.0095],\n",
      "         [-0.0455,  0.0092, -0.0131,  ...,  0.0186,  0.0888,  0.1379],\n",
      "         [ 0.5569, -0.6013,  0.5687,  ..., -0.5004,  0.1755, -0.5053],\n",
      "         ...,\n",
      "         [-0.4860,  0.6538, -0.4850,  ...,  0.4298,  0.1574,  0.2027],\n",
      "         [-0.1176,  0.3214, -0.2791,  ...,  0.0792,  0.0844, -0.1667],\n",
      "         [-0.0203,  0.1706, -0.1092,  ...,  0.0523,  0.0893, -0.0536]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.0.attn.W_V': tensor([[[-0.0368,  0.2840,  0.0080,  ...,  0.0138, -0.0215,  0.0026],\n",
      "         [-0.0492, -0.0047, -0.0950,  ...,  0.0490, -0.0536, -0.0047],\n",
      "         [-0.0030,  0.0122,  0.0313,  ..., -0.0125,  0.0307, -0.0246],\n",
      "         ...,\n",
      "         [ 0.0233,  0.0182, -0.0329,  ...,  0.0374, -0.0079,  0.0692],\n",
      "         [ 0.0329, -0.0352,  0.0626,  ..., -0.0606,  0.0035, -0.0404],\n",
      "         [ 0.0462, -0.0040, -0.0027,  ...,  0.0048, -0.0299, -0.0211]],\n",
      "\n",
      "        [[ 0.0491,  0.0110, -0.0258,  ...,  0.0205,  0.0528, -0.0364],\n",
      "         [-0.0366, -0.0527,  0.0298,  ..., -0.0039, -0.0403,  0.0176],\n",
      "         [ 0.0313,  0.0497, -0.0026,  ...,  0.0046,  0.0507, -0.0096],\n",
      "         ...,\n",
      "         [ 0.0166, -0.0070,  0.0252,  ...,  0.0068, -0.0772, -0.0308],\n",
      "         [ 0.0363,  0.0312, -0.0052,  ...,  0.0941, -0.1136, -0.0455],\n",
      "         [-0.0103,  0.0029, -0.0058,  ..., -0.0011,  0.0232, -0.0229]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.0.attn.b_Q': tensor([[-7.3450e-01,  2.2658e-02, -2.1754e-01,  9.2079e-05, -6.7206e-02,\n",
      "          1.3122e-01, -2.0095e-02, -9.8214e-02, -1.9799e-02, -1.6691e-01,\n",
      "         -6.6339e-02,  6.6268e-01, -7.1435e-01,  2.5396e-02,  2.9886e-02,\n",
      "          4.4598e-01,  4.6834e-02,  3.3166e-02,  5.8334e-01, -4.9047e-01,\n",
      "          3.1613e-01, -6.9854e-01, -8.0567e-02,  1.9502e-02,  5.9672e-02,\n",
      "          6.6111e-01,  4.6811e-03, -1.7506e-01,  2.7946e-03,  8.8584e-02,\n",
      "          1.1607e-02,  9.2398e-02],\n",
      "        [ 1.0759e-01, -2.1333e-02,  8.0773e-02,  8.8640e-03,  7.3034e-01,\n",
      "         -2.6389e-01,  4.3712e-02, -3.1464e-01,  1.4255e-01,  1.9825e-01,\n",
      "         -1.5701e-01, -1.0198e-01,  1.0906e-01, -2.4133e-01,  1.0676e-01,\n",
      "         -5.3419e-01,  4.6896e-02,  1.7395e-01, -8.0666e-02, -1.2935e-01,\n",
      "         -8.5782e-02,  8.7360e-02, -7.4735e-02, -7.8304e-02,  1.1112e-01,\n",
      "         -1.2576e-01,  1.7767e-01, -2.0725e-01,  1.4302e-01, -9.7350e-02,\n",
      "          4.8830e-01, -2.3424e-01]], grad_fn=<SelectBackward0>), 'blocks.0.attn.b_K': tensor([[-1.0310e-03, -1.2802e-03, -2.1654e-03,  1.8854e-03, -9.1076e-04,\n",
      "         -5.5558e-05, -1.4914e-03,  1.3198e-03,  1.0087e-02,  5.3383e-03,\n",
      "         -1.2024e-03,  6.6782e-03, -4.5136e-03,  4.2693e-04, -1.7094e-03,\n",
      "          1.4070e-03,  1.7223e-03,  2.0480e-03,  1.8283e-03, -1.6687e-04,\n",
      "          2.1190e-03, -3.0442e-03,  1.4259e-04,  1.9987e-03,  2.3197e-03,\n",
      "          4.0991e-03, -1.5907e-03,  2.2010e-03, -8.9305e-03,  1.6204e-03,\n",
      "          4.8286e-04,  1.7474e-04],\n",
      "        [-1.1234e-03, -6.2595e-04,  1.2842e-03,  5.3220e-04,  1.9569e-03,\n",
      "         -1.4318e-03,  3.6212e-04, -1.1670e-03,  6.1775e-04,  2.2107e-03,\n",
      "         -6.2119e-04, -1.8225e-03,  1.2964e-03, -1.9381e-03, -1.0952e-05,\n",
      "         -1.9756e-03,  1.3867e-03,  3.1075e-03, -6.7176e-04, -1.2855e-03,\n",
      "         -9.8791e-04,  1.1278e-03,  3.3076e-04,  1.3267e-03, -5.2486e-05,\n",
      "         -1.0612e-04, -4.4176e-04, -2.6850e-04, -1.6325e-04, -1.2149e-03,\n",
      "         -4.0586e-04,  1.1793e-03]], grad_fn=<SelectBackward0>), 'blocks.0.attn.b_V': tensor([[ 0.0022, -0.0406,  0.0069, -0.0460, -0.0028, -0.0015, -0.0032, -0.0053,\n",
      "          0.0064,  0.0028, -0.0001,  0.0012, -0.0450,  0.0023, -0.0426,  0.0041,\n",
      "          0.0046,  0.0009,  0.0367, -0.0060, -0.0035, -0.0105, -0.0005,  0.0044,\n",
      "          0.0020,  0.0106,  0.0086, -0.0055,  0.0110, -0.0031,  0.0044, -0.0080],\n",
      "        [ 0.0016, -0.0063, -0.0065,  0.0008, -0.0026,  0.0015, -0.0029,  0.0033,\n",
      "         -0.0093,  0.0035,  0.0059,  0.0099,  0.0135, -0.0510,  0.0078, -0.0009,\n",
      "         -0.0015, -0.0090,  0.0012,  0.0072,  0.0068, -0.0047, -0.0076,  0.0008,\n",
      "          0.0054,  0.0025, -0.0062,  0.0123,  0.0005, -0.0016, -0.0071, -0.0018]],\n",
      "       grad_fn=<SelectBackward0>), 'blocks.0.attn.W_O': tensor([[[-0.0161,  0.0876,  0.0666,  ...,  0.0341, -0.1271, -0.0378],\n",
      "         [-0.0549,  0.3392, -0.2141,  ...,  0.1894,  0.2997, -0.4211],\n",
      "         [ 0.0107, -0.1336, -0.0009,  ..., -0.0352, -0.0878,  0.1880],\n",
      "         ...,\n",
      "         [-0.0165,  0.0750, -0.0109,  ..., -0.0675, -0.0929,  0.0262],\n",
      "         [-0.0281,  0.0535, -0.0176,  ...,  0.0034, -0.0077, -0.0326],\n",
      "         [ 0.0092,  0.0605,  0.0308,  ..., -0.0378,  0.0171,  0.0852]],\n",
      "\n",
      "        [[ 0.0009,  0.1692,  0.0342,  ...,  0.1173, -0.0820, -0.2615],\n",
      "         [-0.0193,  0.1509, -0.0034,  ...,  0.0731,  0.0561, -0.1608],\n",
      "         [ 0.0220, -0.0019,  0.0094,  ...,  0.1436,  0.0367, -0.2069],\n",
      "         ...,\n",
      "         [-0.0131,  0.1018,  0.0221,  ...,  0.0930, -0.0128, -0.2497],\n",
      "         [-0.0109,  0.4086,  0.2201,  ...,  0.2596, -0.2341,  0.0407],\n",
      "         [ 0.0104, -0.0682,  0.0375,  ..., -0.0005, -0.0608,  0.0046]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.0.attn.b_O': Parameter containing:\n",
      "tensor([ 0.1040, -0.1734,  0.0194,  0.0195, -0.0337,  1.2137,  0.7324, -0.1119,\n",
      "        -0.7458, -0.4622,  0.3017, -0.0578, -0.0266,  0.0168, -0.0054,  0.2598,\n",
      "        -0.4816, -0.0432, -0.1856, -0.5014, -0.0032, -0.0839, -0.0126, -0.1241,\n",
      "        -0.0016,  0.2066,  0.0696, -0.0237,  0.1692, -0.6500, -0.0863,  0.0243,\n",
      "        -0.2067, -0.0846,  0.0729, -0.0069,  0.0958,  0.1085, -1.1999,  0.4428,\n",
      "         0.0025,  0.0291,  0.0805, -0.0828,  0.0578, -0.6740,  0.0096,  0.1098,\n",
      "        -0.0933,  0.0704,  0.2085, -0.0611, -0.1165,  0.0752, -0.0766,  0.1933,\n",
      "         0.0295,  0.7492, -0.0233,  0.2521, -0.1002,  0.1143,  0.0876, -0.2939],\n",
      "       requires_grad=True), 'blocks.0.ln2.w': Parameter containing:\n",
      "tensor([ 8.8878e-01, -1.0503e-03,  2.9598e-01,  6.2974e-03,  1.2059e-01,\n",
      "        -3.8486e-03, -5.5037e-03,  2.9964e-01, -1.2845e-03,  2.2100e-01,\n",
      "        -8.4266e-03,  1.3362e+00,  1.6559e+00,  1.4311e+00,  1.0211e+00,\n",
      "        -1.5166e-04,  4.9982e-01,  8.9285e-02, -8.8403e-04,  6.2400e-01,\n",
      "         9.9571e-01,  6.3039e-01,  3.1529e-03, -5.0443e-04, -2.9194e-03,\n",
      "         1.3565e-03,  4.3686e-01,  4.6846e-02,  1.4627e-01, -2.5907e-03,\n",
      "         1.6616e+00, -1.5297e-03,  6.0124e-01,  3.1948e-01,  7.2032e-01,\n",
      "         5.0965e-04,  7.8466e-01, -1.3235e-03,  1.8175e-01,  1.3509e-03,\n",
      "        -1.7692e-03,  7.4293e-01,  4.7510e-01,  4.4542e-01,  6.8220e-01,\n",
      "         2.8917e-03,  2.5498e+00,  7.4114e-01,  2.2947e-01,  1.2856e-01,\n",
      "        -2.6436e-04,  1.5893e+00,  4.7238e-02,  6.7989e-01,  7.5529e-03,\n",
      "        -7.9871e-04, -1.7052e-03,  3.9849e-01, -2.3014e-03, -5.8039e-03,\n",
      "        -9.7596e-04,  2.7905e-01,  2.4594e-01,  4.9934e-03],\n",
      "       requires_grad=True), 'blocks.0.ln2.b': Parameter containing:\n",
      "tensor([-8.6920e-02,  2.4252e-03, -3.2089e-02,  3.3523e-03,  2.9557e-02,\n",
      "        -2.0548e-03,  1.2097e-04,  7.5678e-03, -1.3764e-03,  1.7498e-01,\n",
      "        -1.9875e-03, -2.0725e-02,  6.6215e-03, -2.1599e-02, -8.0060e-03,\n",
      "         1.7213e-03,  2.7398e-01, -2.0344e-02,  1.0737e-03,  3.3877e-01,\n",
      "        -3.9857e-02, -5.7308e-02,  1.2687e-03, -4.6315e-03,  1.5480e-03,\n",
      "        -1.4640e-05,  3.6337e-02,  6.6256e-03, -8.5578e-02,  3.6072e-03,\n",
      "        -3.0973e-02, -2.5491e-03,  2.3606e-01, -1.2740e-02, -1.4290e-02,\n",
      "        -7.9334e-04,  3.1064e-02,  2.5044e-03,  3.1542e-01,  3.4063e-03,\n",
      "         1.6436e-03,  3.5001e-02,  4.1007e-02, -2.5344e-02, -2.7791e-02,\n",
      "         8.5689e-04, -2.2425e-02, -2.0654e-01,  7.8139e-02, -9.2043e-03,\n",
      "        -1.5312e-03, -2.7315e-02,  1.0131e-02,  2.3859e-02, -1.4675e-03,\n",
      "        -2.0014e-03, -1.0802e-03, -3.2273e-01, -4.3618e-03, -1.3577e-03,\n",
      "        -8.3545e-04,  8.8147e-02, -4.8938e-02, -2.6747e-03],\n",
      "       requires_grad=True), 'blocks.0.mlp.W_in': Parameter containing:\n",
      "tensor([[-0.0206, -0.1463, -0.0798,  ..., -0.0362, -0.0431,  0.0216],\n",
      "        [ 0.0264,  0.0339, -0.0179,  ...,  0.0144, -0.0472, -0.0864],\n",
      "        [ 0.1090, -0.0071,  0.0405,  ...,  0.1059,  0.1108, -0.0048],\n",
      "        ...,\n",
      "        [-0.0173, -0.1327,  0.1588,  ..., -0.0008,  0.0716, -0.0077],\n",
      "        [-0.1821,  0.4523, -0.0662,  ...,  0.0870,  0.0922,  0.0071],\n",
      "        [-0.0142, -0.0385,  0.0191,  ..., -0.0278,  0.0030,  0.1144]],\n",
      "       requires_grad=True), 'blocks.0.mlp.b_in': Parameter containing:\n",
      "tensor([-0.3216, -0.2745, -0.5260, -0.2931, -0.4816, -0.3978, -0.3019, -0.3657,\n",
      "        -0.2995, -0.3125, -0.0243, -0.3660, -0.0646, -0.0159,  0.0691, -0.0173,\n",
      "        -0.4817, -0.5226, -0.3200, -0.4096, -0.3146, -0.3726,  0.2256, -0.0148,\n",
      "        -0.3516, -0.3473,  0.0053, -0.3385, -0.0811,  0.4382, -0.0126, -0.5124,\n",
      "        -0.0198, -0.3397, -0.0035, -0.3399, -0.3313, -0.3675, -0.3200, -0.3404,\n",
      "        -0.0162, -0.2883, -0.5119, -0.2961, -0.3686, -0.2932, -0.3100, -0.3468,\n",
      "        -0.3446, -0.3027,  0.0008,  0.1959, -0.5073, -0.3685, -0.2795, -0.2976,\n",
      "        -0.3359, -0.3828, -0.3343, -0.3246, -0.0091, -0.3217, -0.3610, -0.3319,\n",
      "        -0.4355, -0.0141, -0.0053, -0.3038, -0.0205, -0.0253, -0.4987, -0.3542,\n",
      "        -0.3251, -0.0116, -0.0036, -0.0262, -0.3404, -0.3181, -0.3408,  0.0522,\n",
      "        -0.0462, -0.3551, -0.4032, -0.2540, -0.0088, -0.0143, -0.2798, -0.3732,\n",
      "        -0.1594, -0.4965, -0.1492, -0.3556, -0.3597,  0.0641, -0.0454, -0.0086,\n",
      "        -0.3326, -0.0601, -0.3640, -0.3281, -0.0164, -0.5218, -0.3364, -0.5190,\n",
      "        -0.3359, -0.3534, -0.0147, -0.0382, -0.3004, -0.3294, -0.0810, -0.0535,\n",
      "        -0.5201,  0.0498, -0.3638, -0.5014, -0.7118, -0.4046, -0.2994, -0.3382,\n",
      "         0.0097, -0.0226, -0.3543, -0.2948, -0.4036, -0.3471, -0.3377, -0.3296,\n",
      "        -0.3244, -0.3296, -0.2948, -0.7390, -0.0945, -0.0110, -0.3525, -0.3023,\n",
      "        -0.3217, -0.2742, -0.0921, -0.5030,  0.1181, -0.0090,  0.0760, -0.2727,\n",
      "        -0.3275, -0.5317, -0.3161, -0.3700,  0.1928, -0.0097, -0.3936, -0.0028,\n",
      "        -0.3430, -0.2963, -0.3387,  0.0740, -0.2976, -0.3470, -0.5269, -0.3897,\n",
      "        -0.0163, -0.0081, -0.3563, -0.3411, -0.3271,  0.1108, -0.3850,  0.2837,\n",
      "        -0.3190, -0.5315, -0.3451,  0.2041, -0.3694, -0.4510, -0.4238, -0.4061,\n",
      "        -0.3380, -0.3644,  0.0030, -0.3306, -0.2938, -0.3169, -0.0289, -0.5153,\n",
      "        -0.3865, -0.3341, -0.2484, -0.4177, -0.3304, -0.3073, -0.6096, -0.5282,\n",
      "        -0.3181, -0.1267, -0.0762,  0.0048, -0.3290, -0.4120, -0.3464, -0.3189,\n",
      "        -0.3370,  0.1352, -0.0028, -0.5478, -0.2877, -0.2882, -0.2839, -0.3750,\n",
      "        -0.0352, -0.3295, -0.5249, -0.2814, -0.3785, -0.3449,  0.1236, -0.0712,\n",
      "        -0.5188, -0.0179, -0.0148, -0.3376, -0.0621, -0.5027, -0.0344, -0.3570,\n",
      "        -0.0065,  0.1353, -0.0575, -0.2748, -0.0186, -0.3124, -0.3561, -0.0339,\n",
      "        -0.0061, -0.3634, -0.3378, -0.3275, -0.2792, -0.3251, -0.2786, -0.2849,\n",
      "        -0.0160, -0.0083, -0.3751, -0.3221, -0.3358, -0.0439, -0.3061, -0.0190,\n",
      "        -0.4748, -0.0157,  0.0074, -0.2816, -0.4076, -0.3328, -0.3468, -0.0219],\n",
      "       requires_grad=True), 'blocks.0.mlp.W_out': Parameter containing:\n",
      "tensor([[-0.0073, -0.2453,  0.0771,  ...,  0.2267, -0.1201, -0.0246],\n",
      "        [-0.0235, -0.0712,  0.0759,  ..., -0.0532,  0.0242,  0.0228],\n",
      "        [-0.2037,  0.0489, -0.0385,  ..., -0.0347,  0.1283,  0.0023],\n",
      "        ...,\n",
      "        [-0.0131, -0.2028,  0.0925,  ...,  0.1872, -0.0506,  0.0209],\n",
      "        [-0.0086, -0.1789,  0.0811,  ...,  0.1333, -0.0893, -0.0248],\n",
      "        [ 0.0076,  0.0232, -0.0044,  ..., -0.0179,  0.0084,  0.0402]],\n",
      "       requires_grad=True), 'blocks.0.mlp.b_out': Parameter containing:\n",
      "tensor([ 0.2562, -0.1101,  0.0146,  0.0110, -0.0022,  0.8527,  0.1898, -0.1473,\n",
      "        -0.1324, -0.4197,  0.2426, -0.0051, -0.0521,  0.0069, -0.1081,  0.1757,\n",
      "        -0.7110,  0.0176, -0.1020, -0.7367, -0.1041,  0.1455, -0.0049, -0.0254,\n",
      "        -0.0016,  0.1065,  0.1961,  0.0549,  0.1586, -0.3107,  0.1078,  0.0110,\n",
      "        -0.4541, -0.0113, -0.1829,  0.0050,  0.0825, -0.0289, -0.9048,  0.0804,\n",
      "         0.0095, -0.0112,  0.0487, -0.0323,  0.1587, -0.1710, -0.0639,  0.4168,\n",
      "        -0.0561,  0.0273,  0.1089, -0.0576, -0.0540,  0.1913,  0.0110,  0.0330,\n",
      "         0.0484,  0.8669, -0.0103,  0.2071, -0.0492, -0.0166, -0.1064, -0.0184],\n",
      "       requires_grad=True), 'blocks.1.ln1.w': Parameter containing:\n",
      "tensor([ 5.1768e-01,  4.2739e-01,  1.1900e+00,  2.8159e-01,  6.2368e-01,\n",
      "         9.9028e-02,  4.7497e-01,  1.0507e+00,  5.3784e-01,  5.2261e-01,\n",
      "         4.4989e-01,  5.6626e-01,  7.6070e-01,  3.6385e-01,  3.3211e-01,\n",
      "         6.1789e-01,  2.6931e-01,  1.1395e+00,  4.1425e-01,  1.8685e-01,\n",
      "         2.1806e-01,  5.8951e-01, -1.7710e-03,  7.6124e-01,  5.2560e-04,\n",
      "         8.3004e-01,  5.0126e-01,  1.1661e+00,  7.0823e-01,  5.0081e-01,\n",
      "         5.0169e-01,  8.4558e-01,  4.4737e-01,  4.1332e-01,  7.5798e-01,\n",
      "         4.0961e-01,  8.2368e-02,  3.7563e-01,  9.7750e-02,  1.7118e-03,\n",
      "        -1.9679e-04,  5.8134e-01,  1.1990e-01,  3.0675e-01,  1.0616e+00,\n",
      "         5.6707e-01,  1.0681e+00,  3.4935e-01,  2.4075e-01,  5.1363e-01,\n",
      "         3.7901e-01,  7.8408e-01,  4.7668e-01,  4.3082e-01,  4.7530e-01,\n",
      "         5.2229e-01,  9.4449e-01,  9.4657e-02,  1.3367e+00,  2.6840e-03,\n",
      "         1.1641e+00,  5.2130e-01,  3.7990e-01,  4.3520e-03],\n",
      "       requires_grad=True), 'blocks.1.ln1.b': Parameter containing:\n",
      "tensor([-3.4347e-01,  1.2997e-02,  7.7022e-03, -3.0097e-02,  1.2710e-02,\n",
      "        -3.6454e-01, -8.7124e-02, -2.4970e-02,  1.1138e-01,  4.3955e-01,\n",
      "        -1.7349e-01,  1.7871e-03, -6.5762e-02,  3.1107e-02,  4.6239e-02,\n",
      "        -3.4521e-02,  5.8470e-01, -4.4709e-03,  7.3151e-03,  4.9513e-01,\n",
      "         1.7768e-03, -5.0530e-02,  1.0160e-03,  2.2256e-03,  1.3721e-03,\n",
      "        -8.3820e-02,  6.4626e-02,  8.3511e-02,  1.2610e-02,  1.5054e-01,\n",
      "        -6.9622e-02, -2.9673e-02,  2.1913e-01,  2.0994e-03, -2.7691e-02,\n",
      "        -8.8116e-03,  1.1366e-04, -2.2199e-02,  3.6543e-01,  4.0368e-03,\n",
      "         4.1827e-03, -9.5884e-03,  4.5077e-03, -7.7540e-03, -1.0112e-01,\n",
      "         8.8200e-02,  2.5073e-02, -3.1481e-01,  5.1479e-02, -7.4145e-03,\n",
      "        -7.4814e-02,  1.0900e-01,  2.2165e-02, -9.1366e-02, -1.3250e-02,\n",
      "        -8.7117e-02, -6.0325e-03, -3.2396e-01,  4.7655e-04,  9.1594e-04,\n",
      "         6.4712e-03, -1.0853e-02, -2.7471e-02,  7.5928e-04],\n",
      "       requires_grad=True), 'blocks.1.attn.W_Q': tensor([[[ 0.1797,  0.0178,  0.0549,  ...,  0.1282,  0.2288,  0.1316],\n",
      "         [-0.0582, -0.0747, -0.1067,  ..., -0.0605,  0.1816, -0.0992],\n",
      "         [-0.0130, -0.0183, -0.0270,  ..., -0.0252, -0.0160,  0.0848],\n",
      "         ...,\n",
      "         [-0.0283, -0.2348, -0.2362,  ..., -0.1724,  0.2214, -0.2903],\n",
      "         [-0.3341, -0.0422, -0.1031,  ..., -0.1050, -0.2334, -0.1155],\n",
      "         [-0.0029,  0.0257,  0.0248,  ..., -0.3530, -0.0056,  0.0246]],\n",
      "\n",
      "        [[-0.1162,  0.0239, -0.0353,  ...,  0.0467,  0.0159,  0.0048],\n",
      "         [ 0.0305, -0.0652,  0.4994,  ...,  0.1417,  0.1486,  0.3870],\n",
      "         [ 0.0193, -0.0576,  0.0141,  ..., -0.0163,  0.0042,  0.0147],\n",
      "         ...,\n",
      "         [-0.1195, -0.3358,  0.0290,  ..., -0.1573,  0.2194,  0.2212],\n",
      "         [-0.1512, -0.1638, -0.0149,  ..., -0.3046,  0.1072, -0.0723],\n",
      "         [-0.1078, -0.0285, -0.0193,  ..., -0.0707,  0.1023,  0.0040]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.1.attn.W_K': tensor([[[-0.1090,  0.1368,  0.0253,  ..., -0.0603, -0.3686,  0.1552],\n",
      "         [-0.1256,  0.1724,  0.1547,  ..., -0.0804, -0.0540,  0.0698],\n",
      "         [-0.0175,  0.0146,  0.0076,  ..., -0.0130, -0.0643,  0.0299],\n",
      "         ...,\n",
      "         [-0.4386, -0.0126, -0.0518,  ..., -0.2654, -0.1842, -0.2300],\n",
      "         [ 0.0645, -0.2351, -0.0812,  ...,  0.3059,  0.1952, -0.3114],\n",
      "         [-0.0385,  0.0553, -0.0187,  ..., -0.1255,  0.2074, -0.0572]],\n",
      "\n",
      "        [[-0.0628, -0.0326,  0.0425,  ..., -0.0317,  0.0521,  0.0479],\n",
      "         [-0.0961, -0.1799,  0.1025,  ..., -0.2947,  0.2435,  0.0767],\n",
      "         [ 0.0409,  0.0694, -0.0115,  ...,  0.0938, -0.0507, -0.0025],\n",
      "         ...,\n",
      "         [-0.2888, -0.6353,  0.4576,  ..., -0.7747,  0.5892,  0.5717],\n",
      "         [ 0.0828,  0.4623, -0.2300,  ...,  0.6772, -0.2618, -0.2292],\n",
      "         [-0.1239, -0.0110,  0.1260,  ...,  0.0328,  0.0455,  0.0873]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.1.attn.W_V': tensor([[[ 0.1172,  0.0100,  0.0161,  ..., -0.0281, -0.0007, -0.0105],\n",
      "         [ 0.0179,  0.0373,  0.0163,  ...,  0.0202, -0.0167, -0.0092],\n",
      "         [ 0.6191,  0.0257, -0.0502,  ..., -0.0312,  0.2283,  0.0271],\n",
      "         ...,\n",
      "         [-0.0652,  0.0092, -0.0418,  ..., -0.0063,  0.1144, -0.0390],\n",
      "         [-0.0233, -0.0200,  0.0092,  ..., -0.0717, -0.0741, -0.0609],\n",
      "         [ 0.0551,  0.0479, -0.0081,  ..., -0.0050, -0.0298,  0.0242]],\n",
      "\n",
      "        [[ 0.3394, -0.0309,  0.1120,  ..., -0.0334, -0.0184, -0.0325],\n",
      "         [ 0.3527, -0.0047,  0.0490,  ..., -0.0510, -0.0116, -0.0224],\n",
      "         [ 0.0923,  0.0314,  0.0448,  ...,  0.0030, -0.0149,  0.0055],\n",
      "         ...,\n",
      "         [ 0.1101, -0.0378,  0.3863,  ..., -0.1712,  0.1169, -0.0059],\n",
      "         [-0.0944, -0.0153, -0.3742,  ..., -0.0639, -0.0090,  0.0790],\n",
      "         [-0.0504,  0.0157,  0.0476,  ...,  0.0322,  0.0133,  0.0319]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.1.attn.b_Q': tensor([[ 4.5945e-02,  3.9388e-02,  4.4823e-02, -6.5376e-02,  1.4424e-02,\n",
      "         -9.2145e-03, -2.5299e-02,  3.6712e-02, -2.4780e-02, -4.1981e-02,\n",
      "         -3.5314e-02,  9.8426e-03, -2.0223e-02, -1.3641e-02,  1.2867e-04,\n",
      "         -3.5859e-02,  7.8582e-02, -4.2281e-02, -4.9112e-02,  9.2796e-03,\n",
      "         -2.3148e-02,  2.2149e-02, -1.0969e-01, -6.4357e-03,  2.9161e-02,\n",
      "          4.8018e-02, -2.1747e-02,  5.9850e-02,  3.3405e-02, -9.2310e-03,\n",
      "         -2.1634e-02,  4.3360e-02],\n",
      "        [-3.4786e-02, -3.1551e-01,  3.2079e-01, -3.2510e-01, -4.7656e-02,\n",
      "          2.6497e-01, -2.1268e-02, -4.7316e-01, -5.5489e-03,  5.2137e-03,\n",
      "         -5.6164e-02,  3.3630e-02, -1.5387e-03, -4.3996e-02,  2.5990e-01,\n",
      "          1.0305e-01, -2.6422e-03,  2.4199e-02, -1.3415e-01, -9.1816e-03,\n",
      "         -1.7504e-03,  1.9746e-01, -6.5754e-02,  3.6679e-02,  3.6362e-02,\n",
      "         -1.6047e-02, -7.0898e-03, -6.2203e-02, -1.0435e-01, -3.4906e-01,\n",
      "          1.0521e-01,  1.9741e-01]], grad_fn=<SelectBackward0>), 'blocks.1.attn.b_K': tensor([[ 9.1943e-03,  1.9883e-03,  5.0231e-04, -2.3600e-03,  1.6615e-03,\n",
      "          7.0520e-04, -9.7356e-05, -1.0997e-03,  4.6530e-04,  1.3678e-03,\n",
      "         -3.0491e-03, -3.8077e-03,  4.0191e-03, -6.5266e-03, -1.1314e-04,\n",
      "         -1.8408e-03,  1.5020e-03, -1.7127e-03,  5.4749e-04,  2.8584e-03,\n",
      "         -1.8865e-03,  2.2085e-03,  1.2832e-04, -8.8720e-04, -6.7868e-03,\n",
      "         -1.8592e-03, -5.9211e-04, -9.5599e-04, -2.4196e-03, -7.3557e-04,\n",
      "          8.9713e-04,  7.8391e-05],\n",
      "        [ 7.7909e-04,  4.3214e-04,  5.9711e-04,  1.5045e-03,  1.2172e-03,\n",
      "         -2.3784e-03,  6.9454e-03,  6.3290e-03,  3.1318e-03,  4.1423e-04,\n",
      "          1.0667e-02, -2.8836e-03, -1.1894e-02,  8.7162e-03, -3.0470e-03,\n",
      "         -1.3188e-03, -3.5705e-03,  1.3919e-02, -5.4018e-04, -1.8410e-03,\n",
      "          7.2115e-03, -2.5966e-03,  4.3178e-03, -2.8811e-03, -1.8123e-03,\n",
      "          1.7635e-04, -2.7542e-03,  1.4970e-03, -2.1310e-03,  3.7172e-03,\n",
      "         -3.1637e-03,  6.1410e-04]], grad_fn=<SelectBackward0>), 'blocks.1.attn.b_V': tensor([[ 0.0055, -0.0039,  0.0047, -0.0038,  0.0052,  0.0048, -0.0008, -0.0039,\n",
      "         -0.0047,  0.0079,  0.0025, -0.0044,  0.0011, -0.0062, -0.0069,  0.0043,\n",
      "         -0.0035,  0.0014, -0.0035,  0.0053,  0.0106,  0.0017, -0.0040, -0.0020,\n",
      "          0.0004,  0.0003, -0.0019, -0.0042, -0.0095,  0.0043,  0.0032, -0.0003],\n",
      "        [ 0.0106,  0.0077, -0.0291,  0.0099,  0.0037,  0.0254, -0.0203,  0.0005,\n",
      "         -0.0133, -0.0105, -0.0115, -0.0006,  0.0007, -0.0394,  0.0150, -0.0119,\n",
      "         -0.0121,  0.0049, -0.0103,  0.0043,  0.0284, -0.0176,  0.0045,  0.0214,\n",
      "          0.0064, -0.0035,  0.0161,  0.0082, -0.0003,  0.0079, -0.0009,  0.0059]],\n",
      "       grad_fn=<SelectBackward0>), 'blocks.1.attn.W_O': tensor([[[ 1.3311e-03, -5.4852e-02,  5.0317e-02,  ...,  1.9439e-02,\n",
      "          -2.1799e-01, -2.0121e-01],\n",
      "         [ 6.7085e-03, -8.8827e-02,  2.3519e-02,  ...,  5.4804e-02,\n",
      "          -3.7089e-02, -9.2147e-02],\n",
      "         [ 3.7176e-02,  1.2862e-03, -4.3946e-02,  ..., -6.6189e-02,\n",
      "          -2.2121e-03,  9.2259e-02],\n",
      "         ...,\n",
      "         [ 4.5284e-02, -9.1489e-02, -3.0500e-03,  ..., -5.3381e-02,\n",
      "           8.9026e-02,  7.0356e-03],\n",
      "         [ 8.6107e-02,  1.2427e-01,  2.2732e-02,  ..., -2.6773e-01,\n",
      "           4.6236e-02,  2.6008e-01],\n",
      "         [-1.3736e-02,  6.3445e-02, -1.5024e-02,  ...,  4.6043e-02,\n",
      "          -1.8633e-02,  5.1189e-02]],\n",
      "\n",
      "        [[-2.9876e-01, -7.0582e-01, -4.6373e-02,  ..., -2.2768e-01,\n",
      "           1.6066e-01, -8.9047e-01],\n",
      "         [-7.1168e-04, -4.1760e-02, -3.0537e-02,  ...,  7.2226e-02,\n",
      "          -3.0530e-02, -2.0044e-01],\n",
      "         [-2.7612e-02, -1.0635e-01,  4.7273e-03,  ..., -1.3460e-01,\n",
      "           1.0476e-01,  1.7791e-01],\n",
      "         ...,\n",
      "         [-2.7943e-03,  1.1755e-02,  6.7025e-03,  ...,  2.2270e-03,\n",
      "          -1.1417e-02, -9.6031e-02],\n",
      "         [ 4.9064e-02,  6.6787e-02, -3.8814e-02,  ..., -1.3359e-02,\n",
      "          -7.4418e-02,  3.5689e-02],\n",
      "         [ 8.5040e-03, -9.0247e-03,  6.7550e-03,  ...,  1.8306e-02,\n",
      "           4.1304e-02, -8.5271e-02]]], grad_fn=<ReshapeAliasBackward0>), 'blocks.1.attn.b_O': Parameter containing:\n",
      "tensor([ 2.7172e-01, -4.7230e-02, -8.0848e-03,  1.1651e-02, -7.2381e-03,\n",
      "         8.2817e-01,  7.3189e-02, -8.0616e-02, -4.3992e-02, -5.8904e-01,\n",
      "         3.4323e-01,  1.2139e-02,  4.2314e-02,  1.4352e-02, -4.3115e-02,\n",
      "         6.1159e-02, -7.5788e-01,  4.0412e-03,  5.6284e-02, -8.1100e-01,\n",
      "        -2.1661e-02,  1.3212e-01, -2.4155e-03, -2.2346e-04, -4.2528e-03,\n",
      "         3.4854e-01,  1.4003e-01, -2.3706e-02,  9.1379e-02, -3.1043e-01,\n",
      "         4.1335e-02,  1.2475e-02, -4.2178e-01,  2.1993e-02, -9.9930e-02,\n",
      "         6.9396e-03,  7.0427e-02,  1.9949e-02, -8.9570e-01,  7.5970e-02,\n",
      "         1.0138e-02, -9.6693e-04,  4.0078e-02, -5.1400e-02,  1.8104e-01,\n",
      "        -6.5333e-02, -4.9323e-02,  4.5186e-01, -8.0382e-02,  1.0840e-01,\n",
      "         7.9477e-02, -1.7084e-01, -4.1564e-02,  1.6439e-01,  5.7939e-02,\n",
      "         2.2126e-02,  9.6119e-03,  8.4714e-01, -3.3867e-03,  1.4921e-01,\n",
      "        -7.5788e-03, -4.1515e-02, -5.3082e-02,  7.6248e-02],\n",
      "       requires_grad=True), 'blocks.1.ln2.w': Parameter containing:\n",
      "tensor([ 5.1131e-01,  4.7091e-01,  6.3149e-03,  1.8518e-03,  7.4866e-01,\n",
      "         4.4929e-01,  6.8618e-01,  9.2830e-01,  6.6022e-01,  5.0425e-01,\n",
      "         7.7877e-01, -1.6974e-02,  1.4766e-01,  5.1209e-01,  6.6388e-01,\n",
      "         4.9877e-01,  4.1793e-01,  1.9637e-03,  6.2421e-04,  6.4612e-01,\n",
      "         6.4901e-01,  1.6376e-02,  6.1035e-01,  4.3087e-03,  1.9326e-04,\n",
      "         4.0379e-01,  3.1424e-02,  1.0194e+00,  1.0463e+00,  9.1037e-01,\n",
      "         3.3342e-04,  2.3218e-01,  6.4314e-01, -7.1431e-03,  5.6865e-01,\n",
      "        -9.5631e-03, -2.6057e-03,  4.3167e-01,  4.4672e-01,  3.2382e-04,\n",
      "         3.1815e-03, -5.7422e-03,  1.3449e-02, -4.8441e-03,  1.2707e+00,\n",
      "         7.5327e-01,  7.8208e-01,  5.4336e-01,  3.4013e-01,  8.6428e-01,\n",
      "         2.4531e-01,  6.3070e-01,  4.2695e-01,  1.3620e-02, -1.0581e-02,\n",
      "         4.8999e-01, -6.6343e-04,  4.5619e-01,  3.1969e-01, -2.1354e-03,\n",
      "         4.5907e-03,  9.6635e-01,  9.3786e-03,  1.2364e-03],\n",
      "       requires_grad=True), 'blocks.1.ln2.b': Parameter containing:\n",
      "tensor([-2.6641e-01,  5.8485e-02,  4.3712e-03, -5.2075e-03, -1.4686e-03,\n",
      "        -5.4329e-01, -1.4303e-01, -6.4227e-02,  1.7909e-01,  3.7837e-01,\n",
      "        -2.6286e-01, -4.1960e-03, -1.4884e-03,  2.2810e-02,  7.9454e-02,\n",
      "        -6.0388e-02,  4.9047e-01, -7.5552e-04,  1.9934e-03,  5.6443e-01,\n",
      "         4.5821e-02,  3.4459e-03, -5.9747e-02,  2.7244e-03, -6.0741e-03,\n",
      "        -5.9783e-02,  2.9466e-03,  9.6516e-02, -7.8810e-02,  2.1842e-01,\n",
      "         8.2768e-04, -3.7777e-02,  3.2517e-01,  3.1613e-03, -5.4748e-02,\n",
      "        -2.7703e-03, -2.2839e-03, -1.1068e-02,  6.0948e-01,  2.9569e-03,\n",
      "        -2.3931e-03, -2.8831e-05,  4.5460e-03, -3.3635e-03, -8.1401e-02,\n",
      "         1.6445e-01,  5.4915e-02, -3.7319e-01,  7.0302e-02, -7.1153e-02,\n",
      "        -9.0679e-02,  6.7646e-02,  4.9970e-02,  3.8144e-03, -4.1206e-03,\n",
      "        -5.2946e-02, -1.1364e-03, -5.1015e-01,  4.7987e-02,  2.0039e-03,\n",
      "        -3.9489e-03,  9.9657e-02, -4.4866e-03, -3.0422e-03],\n",
      "       requires_grad=True), 'blocks.1.mlp.W_in': Parameter containing:\n",
      "tensor([[-0.0026, -0.0035, -0.1796,  ..., -0.0196, -0.0087,  0.1021],\n",
      "        [ 0.0139,  0.0504,  0.0481,  ...,  0.0252, -0.0282,  0.1132],\n",
      "        [ 0.0051,  0.0258, -0.0120,  ...,  0.0119, -0.0561, -0.0061],\n",
      "        ...,\n",
      "        [-0.0373, -0.0414, -0.2829,  ...,  0.0209, -0.0464, -0.1194],\n",
      "        [-0.0100,  0.0133,  0.0190,  ...,  0.0641, -0.0042, -0.0124],\n",
      "        [ 0.0367, -0.0029,  0.0053,  ...,  0.0360,  0.0097,  0.0059]],\n",
      "       requires_grad=True), 'blocks.1.mlp.b_in': Parameter containing:\n",
      "tensor([-5.5832e-03, -2.7507e-02, -8.5084e-02, -3.3090e-03, -1.0617e-01,\n",
      "        -2.3616e-02, -1.1461e-01, -8.2880e-03, -4.8350e-01,  1.7142e-04,\n",
      "         6.0132e-04,  7.9804e-03, -4.9937e-01, -2.7614e-01, -4.7108e-02,\n",
      "        -1.5011e-04, -1.4884e-02, -3.2401e-02, -3.6912e-03, -2.2059e-04,\n",
      "        -2.6733e-03, -1.6042e-01, -3.2016e-03, -1.4676e-02, -5.9478e-03,\n",
      "        -1.2336e-01, -2.6759e-02, -3.0189e-03, -5.9486e-04, -3.1939e-02,\n",
      "        -1.4022e-01, -2.7675e-01, -4.9917e-02, -1.0465e-01, -1.1152e-02,\n",
      "        -5.2709e-01, -2.6783e-01, -4.7106e-03, -5.0333e-03,  5.3864e-03,\n",
      "         4.1696e-03, -4.3664e-01,  7.9017e-03, -1.5815e-02, -3.1968e-03,\n",
      "        -8.4788e-03, -1.1947e-02, -1.6798e-02, -3.4890e-03, -5.6698e-01,\n",
      "         1.2499e-03, -1.7719e-03, -4.5851e-01, -1.9613e-02, -1.1189e-02,\n",
      "        -3.6020e-01, -2.3626e-02, -1.2693e-02, -1.3637e-02, -1.1046e-01,\n",
      "        -1.8338e-02, -2.3640e-02,  3.6702e-03, -1.4144e-03,  2.6339e-03,\n",
      "        -1.2573e-01, -2.2447e-01,  2.0398e-03, -1.3517e-02, -1.4828e-02,\n",
      "        -1.7635e-03, -7.9380e-02,  8.3478e-03, -2.5637e-01, -2.8560e-02,\n",
      "        -5.6702e-01, -4.0259e-02,  1.7075e-02, -7.1420e-03, -7.5106e-03,\n",
      "        -7.1264e-02, -5.4782e-03, -3.8430e-01, -7.4675e-03, -4.7487e-01,\n",
      "        -3.0061e-03, -5.5428e-03, -6.0857e-03, -2.2048e-02, -1.8990e-02,\n",
      "        -6.2236e-03, -1.3061e-02, -5.9085e-03, -1.0279e-02, -2.3815e-02,\n",
      "        -1.6787e-02, -2.1971e-03, -6.2857e-02, -1.2868e-02, -3.1227e-01,\n",
      "        -1.7470e-03, -7.4228e-03, -1.1546e-02, -1.7059e-02, -1.1947e-02,\n",
      "        -1.0869e-01, -2.5644e-02, -7.6384e-03,  4.1317e-03,  7.3425e-03,\n",
      "        -1.4135e-01, -6.1694e-02, -2.9895e-03, -1.3337e-02, -1.0244e-02,\n",
      "        -2.7105e-01, -1.7637e-01, -5.4606e-03, -3.1118e-02, -1.3837e-02,\n",
      "        -1.1728e-02, -3.5391e-03, -1.8768e-04, -8.1964e-02, -1.0829e-02,\n",
      "        -1.3237e-02, -2.0710e-02, -4.6480e-01, -2.5471e-01, -9.6189e-03,\n",
      "        -1.3610e-02, -4.3006e-03, -1.6884e-02, -1.6120e-02, -4.6836e-01,\n",
      "        -9.3904e-03, -3.9184e-01, -8.7397e-03, -1.2579e-02, -1.8724e-02,\n",
      "        -6.1368e-04, -1.0926e-02, -1.6410e-02, -1.8127e-02, -1.4170e-02,\n",
      "        -1.1811e-03, -4.6909e-01, -5.9631e-03, -1.1283e-02, -4.3592e-01,\n",
      "        -1.7184e-02, -8.2213e-03, -1.7785e-02, -3.7155e-03, -7.9290e-02,\n",
      "        -6.9036e-03, -9.1845e-03, -4.6554e-03, -8.0073e-03,  2.0568e-04,\n",
      "        -1.6994e-01, -3.6775e-02, -7.7925e-02, -7.9767e-03, -2.4776e-02,\n",
      "        -8.9117e-02, -2.5293e-02, -1.0289e-02, -1.0070e-02, -5.8782e-01,\n",
      "        -1.0984e-02, -1.3253e-03,  1.3625e-03, -6.0424e-02,  2.5592e-03,\n",
      "        -7.3577e-03, -1.9114e-03, -1.7591e-02, -7.1493e-02, -1.4345e-01,\n",
      "        -1.8034e-02, -2.5117e-02, -2.3587e-02, -2.8887e-02, -9.1372e-03,\n",
      "        -1.6644e-02, -2.4435e-03, -1.4902e-02, -1.6438e-02,  3.7903e-03,\n",
      "        -1.2225e-02, -2.3923e-03, -2.2595e-03, -1.3416e-02, -8.5368e-02,\n",
      "         1.7937e-02, -2.1373e-02, -4.2406e-01, -7.4515e-03,  3.9059e-03,\n",
      "        -9.6086e-02, -3.7627e-02, -1.5428e-02, -6.6781e-02, -1.0469e-02,\n",
      "        -1.4916e-01, -9.4999e-03, -2.3691e-01, -9.9311e-03, -1.6713e-02,\n",
      "        -1.5767e-02, -6.8238e-03, -5.4051e-02, -1.2186e-01, -5.9505e-01,\n",
      "        -1.6631e-02,  1.7264e-03, -1.3248e-02,  3.3839e-03, -1.5022e-02,\n",
      "        -3.9813e-03, -5.5686e-03, -1.3774e-02, -1.0724e-02, -2.6254e-02,\n",
      "        -4.7012e-01, -2.1496e-03, -3.9241e-03, -1.4612e-02, -9.8662e-03,\n",
      "        -4.5421e-03, -4.5015e-02, -2.4351e-02, -1.9321e-01, -7.7660e-03,\n",
      "         5.1458e-04,  4.3925e-04, -1.6440e-02, -5.7305e-03, -1.5017e-02,\n",
      "        -4.2624e-01, -2.3816e-02, -3.0954e-02, -5.4904e-01, -3.4650e-03,\n",
      "        -4.7582e-01, -1.3603e-01, -2.0166e-02, -6.9095e-03, -2.7467e-02,\n",
      "        -5.0978e-01,  3.2806e-03, -3.3775e-03, -3.3254e-03, -1.7759e-02,\n",
      "        -5.3313e-01], requires_grad=True), 'blocks.1.mlp.W_out': Parameter containing:\n",
      "tensor([[ 1.9283e-02, -2.7795e-02, -4.3759e-03,  ..., -1.7830e-02,\n",
      "         -3.9446e-02,  7.5997e-02],\n",
      "        [ 9.7730e-03, -2.9004e-02,  3.7270e-03,  ...,  5.5813e-03,\n",
      "         -8.1954e-02,  3.7910e-02],\n",
      "        [-2.0495e-01, -4.0831e-01,  1.0102e-01,  ...,  4.6915e-02,\n",
      "          3.9939e-01,  2.8867e-01],\n",
      "        ...,\n",
      "        [-1.7042e-02,  2.0862e-02, -1.8993e-02,  ...,  1.4928e-02,\n",
      "         -5.9715e-02, -1.2499e-02],\n",
      "        [ 1.4285e-02, -4.2563e-02, -5.1256e-03,  ..., -4.9457e-05,\n",
      "         -6.5738e-02,  4.9553e-02],\n",
      "        [-9.4522e-02,  2.5943e-02, -1.5355e-02,  ...,  1.2307e-02,\n",
      "          3.3861e-02, -9.7824e-02]], requires_grad=True), 'blocks.1.mlp.b_out': Parameter containing:\n",
      "tensor([ 1.9214e-01,  7.8654e-03, -1.7330e-03,  7.3702e-03, -2.4867e-02,\n",
      "         7.5256e-01,  5.1154e-02, -3.4633e-02, -1.2519e-01, -5.3487e-01,\n",
      "         4.5034e-01, -2.7120e-03,  3.5410e-02,  5.2376e-02, -1.2144e-02,\n",
      "         3.4885e-02, -6.8518e-01,  1.0105e-02,  4.5385e-02, -7.7818e-01,\n",
      "         6.5448e-03,  9.1774e-02,  6.7469e-03,  1.8468e-03, -1.4983e-03,\n",
      "         3.5286e-01,  1.7104e-01, -9.2820e-02,  6.8503e-02, -3.8311e-01,\n",
      "         3.1571e-02,  3.0531e-03, -3.8771e-01,  1.0786e-02, -6.9697e-02,\n",
      "         6.2181e-03,  9.1316e-02,  4.2772e-02, -8.0847e-01,  4.2766e-02,\n",
      "         7.9792e-03, -1.3801e-03,  2.9088e-02, -9.4463e-02,  4.1963e-01,\n",
      "        -9.9726e-02, -6.8519e-02,  3.9610e-01, -2.2227e-02,  2.4599e-01,\n",
      "        -5.5454e-02, -2.4644e-01,  4.8945e-04,  1.5985e-01,  2.5759e-02,\n",
      "         9.5151e-03,  4.4256e-03,  7.6597e-01, -6.2013e-03,  8.2078e-02,\n",
      "        -2.0991e-03, -9.7378e-02, -6.2111e-02,  7.5780e-02],\n",
      "       requires_grad=True), 'blocks.2.ln1.w': Parameter containing:\n",
      "tensor([ 6.2582e-01,  5.1364e-01,  1.1743e+00,  1.2825e-02,  2.6978e-01,\n",
      "         1.6849e-01,  6.1303e-01,  7.0949e-01,  2.1878e-01,  3.9395e-01,\n",
      "         5.7361e-01,  5.7331e-01,  7.4595e-01,  5.7909e-01,  6.3332e-01,\n",
      "         8.2716e-01,  2.9675e-01,  1.1341e+00,  2.6669e-01,  3.2941e-01,\n",
      "         6.5946e-01,  5.8286e-01,  1.3451e-03,  8.9587e-01, -2.6364e-03,\n",
      "         6.0669e-01,  7.3207e-01,  7.6405e-01,  1.1255e+00,  4.8718e-01,\n",
      "         8.8960e-01,  1.0199e+00,  5.2953e-01,  5.7305e-01,  9.9862e-01,\n",
      "         5.3990e-01,  3.9793e-01,  2.4281e-01,  1.2386e-01,  4.0043e-01,\n",
      "         1.1272e-01,  3.8675e-01,  1.2298e-01,  3.2742e-01,  6.3601e-01,\n",
      "         3.4877e-01,  7.4105e-01,  5.4174e-01,  2.8269e-01,  8.4556e-01,\n",
      "         8.9179e-01,  7.1660e-01,  7.7288e-01,  2.0253e-01,  2.1411e-02,\n",
      "         5.8168e-01,  9.2818e-01,  1.8744e-01,  1.3523e+00,  5.9731e-02,\n",
      "         1.0899e+00,  1.0022e+00,  6.0575e-01,  8.9152e-03],\n",
      "       requires_grad=True), 'blocks.2.ln1.b': Parameter containing:\n",
      "tensor([-3.3677e-01,  1.0836e-03,  3.5255e-03,  6.3900e-04,  5.2677e-03,\n",
      "        -6.2507e-01, -1.0073e-01,  8.8003e-02,  2.9627e-02,  4.5954e-01,\n",
      "        -3.8939e-01,  2.0414e-03, -7.0320e-02,  3.8513e-04,  5.0548e-02,\n",
      "        -2.3192e-02,  7.5229e-01, -2.4454e-02, -6.2022e-03,  9.4998e-01,\n",
      "        -2.4769e-02, -6.2909e-02,  6.7471e-04,  1.2986e-02,  1.2538e-03,\n",
      "        -2.2000e-01, -6.4803e-02,  2.5830e-02,  1.8909e-02,  2.6634e-01,\n",
      "        -7.0359e-02, -1.0140e-02,  3.6556e-01, -3.9554e-03,  9.2620e-02,\n",
      "        -3.5733e-02, -3.6656e-02, -1.8573e-02,  4.7562e-01,  3.0407e-02,\n",
      "        -5.0004e-03,  2.2397e-02, -4.6493e-03,  1.8676e-02, -2.9210e-01,\n",
      "         2.9310e-02, -1.5615e-02, -5.3565e-01,  5.0081e-02, -6.6537e-02,\n",
      "        -6.0396e-02,  1.7044e-01, -2.8026e-02, -7.6630e-02, -1.7712e-03,\n",
      "        -8.9091e-02, -1.1181e-02, -6.5886e-01,  1.4391e-04, -3.1524e-02,\n",
      "         7.3203e-03, -3.1001e-02,  1.7093e-02, -1.8309e-03],\n",
      "       requires_grad=True), 'blocks.2.attn.W_Q': tensor([[[ 0.0278, -0.2112,  0.0982,  ..., -0.0098,  0.0146,  0.0979],\n",
      "         [ 0.1758, -0.6558, -0.1430,  ...,  0.2258,  0.1138,  0.0274],\n",
      "         [ 0.0141,  0.1764,  0.0324,  ..., -0.0222, -0.0527,  0.0300],\n",
      "         ...,\n",
      "         [-0.1644, -0.1100,  0.2146,  ..., -0.2100, -0.1611,  0.2758],\n",
      "         [-0.2998,  0.5834, -0.0340,  ..., -0.1696,  0.0735,  0.0589],\n",
      "         [-0.0351,  0.0230, -0.0508,  ..., -0.0093,  0.0069,  0.0568]],\n",
      "\n",
      "        [[-0.2432, -0.0121,  0.1552,  ...,  0.1036,  0.1360, -0.0614],\n",
      "         [ 0.2287, -0.2097, -0.2166,  ...,  0.0286,  0.3979, -0.1156],\n",
      "         [-0.0048,  0.0138, -0.0014,  ..., -0.0140, -0.0491, -0.0203],\n",
      "         ...,\n",
      "         [ 0.3198,  0.1562, -0.2943,  ...,  0.0453, -0.3076, -0.1169],\n",
      "         [-0.1139, -0.2289,  0.1320,  ...,  0.2477,  0.1599,  0.0203],\n",
      "         [-0.0112,  0.0221, -0.0079,  ...,  0.0273,  0.0839,  0.0409]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.2.attn.W_K': tensor([[[-0.0980, -0.1638, -0.0553,  ..., -0.0249, -0.4417,  0.2241],\n",
      "         [ 0.2531, -0.6445, -0.2422,  ...,  0.3567,  0.0482, -0.1804],\n",
      "         [ 0.0122, -0.0905,  0.0465,  ..., -0.0215, -0.0176,  0.0013],\n",
      "         ...,\n",
      "         [ 0.2030, -0.0183,  0.2935,  ..., -0.0603, -0.1636, -0.0336],\n",
      "         [-0.1704, -0.0327, -0.2927,  ...,  0.1235,  0.0669, -0.0324],\n",
      "         [ 0.1718,  0.0273, -0.1234,  ...,  0.1752,  0.1389, -0.1679]],\n",
      "\n",
      "        [[ 0.3760,  0.0736, -0.3225,  ...,  0.0150, -0.1658, -0.1377],\n",
      "         [-0.0703,  0.0781,  0.0227,  ..., -0.0569,  0.1219,  0.0722],\n",
      "         [ 0.0992,  0.0305, -0.0978,  ..., -0.0497, -0.0346, -0.0296],\n",
      "         ...,\n",
      "         [-0.4540,  0.0507,  0.4688,  ..., -0.1545, -0.1680,  0.3972],\n",
      "         [ 0.2008,  0.2264, -0.2202,  ..., -0.1848, -0.1191, -0.2433],\n",
      "         [-0.0597, -0.2068,  0.0659,  ...,  0.1716,  0.2167,  0.1498]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>), 'blocks.2.attn.W_V': tensor([[[ 9.1946e-03,  2.6794e-02, -9.5404e-02,  ...,  6.1597e-02,\n",
      "          -2.6080e-02,  5.4479e-02],\n",
      "         [ 1.1110e-02,  3.5775e-02, -4.5717e-02,  ...,  2.4260e-02,\n",
      "          -9.1627e-02, -3.6277e-03],\n",
      "         [-4.2995e-02,  3.3803e-02, -7.4598e-01,  ...,  6.6461e-01,\n",
      "          -1.8452e-03,  2.1963e-02],\n",
      "         ...,\n",
      "         [ 1.4667e-01, -1.2867e-01, -1.5935e-01,  ...,  1.9203e-01,\n",
      "           3.2225e-02, -5.3823e-02],\n",
      "         [ 4.8748e-02, -5.6077e-02, -1.8988e-02,  ...,  1.2336e-01,\n",
      "           2.9630e-02,  1.1947e-02],\n",
      "         [ 2.0773e-02,  1.8011e-02, -3.7308e-03,  ...,  9.6935e-03,\n",
      "          -4.3241e-03, -2.9694e-02]],\n",
      "\n",
      "        [[-1.3258e-01,  2.5774e-01, -8.4247e-03,  ...,  2.3087e-01,\n",
      "          -2.1352e-04, -3.6857e-03],\n",
      "         [-2.2738e-02,  1.1754e-01, -8.9447e-02,  ...,  1.0118e-01,\n",
      "           5.2850e-02, -3.1132e-03],\n",
      "         [ 3.7422e-01, -5.6966e-03, -3.7749e-01,  ..., -4.9820e-03,\n",
      "           1.3062e-03, -5.0790e-01],\n",
      "         ...,\n",
      "         [ 4.2925e-02,  1.6579e-02,  2.4253e-01,  ..., -6.0682e-03,\n",
      "           4.0965e-02,  1.4467e-01],\n",
      "         [ 1.8032e-01,  2.3196e-02,  4.0500e-02,  ...,  6.4719e-02,\n",
      "           1.1906e-01, -6.7728e-03],\n",
      "         [-1.7015e-02, -1.0633e-02, -3.1285e-02,  ..., -2.0742e-02,\n",
      "          -1.6775e-02,  2.7928e-02]]], grad_fn=<ReshapeAliasBackward0>), 'blocks.2.attn.b_Q': tensor([[-0.0054, -0.0589,  0.0698,  0.0243, -0.0404, -0.0911, -0.0822,  0.1527,\n",
      "         -0.0051, -0.1351, -0.0921,  0.7009,  0.0181, -0.0422,  0.6048,  0.7372,\n",
      "         -0.5286, -0.3358, -0.6835, -0.0269,  0.7167, -0.1088, -0.6691, -0.1246,\n",
      "          0.3042, -0.0182, -0.7018,  0.7256,  0.0543, -0.0402, -0.3180,  0.1277],\n",
      "        [-0.5319, -0.0159,  0.4894,  0.0143, -0.1418,  0.0390,  0.5765, -0.5280,\n",
      "         -0.2071, -0.3819,  0.0997, -0.3678, -0.6047,  0.3868, -0.0642,  0.5731,\n",
      "         -0.4964, -0.1152, -0.1718, -0.6159, -0.1285,  0.0180, -0.0989, -0.5019,\n",
      "         -0.5967,  0.5345,  0.0110,  0.7121, -0.3099,  0.0478,  0.2086,  0.2447]],\n",
      "       grad_fn=<SelectBackward0>), 'blocks.2.attn.b_K': tensor([[ 8.6646e-04,  9.5391e-03, -9.3056e-04, -1.7372e-03, -6.3065e-04,\n",
      "         -3.7225e-03, -2.8397e-03,  3.0787e-03,  2.1423e-03, -1.9892e-03,\n",
      "         -1.1810e-03,  1.3202e-02,  7.6146e-04,  8.6607e-05,  7.2542e-03,\n",
      "          2.6926e-03, -2.2901e-03,  1.4910e-03, -1.3354e-02,  4.6201e-04,\n",
      "          7.5685e-04, -2.6087e-03, -8.1640e-03, -3.1541e-03,  2.8969e-03,\n",
      "         -1.2331e-03, -2.1864e-03, -3.3328e-03, -3.5401e-03, -3.4870e-03,\n",
      "         -4.5630e-03,  2.3168e-03],\n",
      "        [-8.1872e-03, -3.3140e-03,  5.5696e-03, -2.5868e-04,  1.4123e-04,\n",
      "         -4.6433e-04,  5.2954e-03, -5.4292e-03, -3.7770e-03,  8.7041e-04,\n",
      "          2.2842e-04, -4.8202e-03, -4.8045e-03,  3.6701e-03, -9.3499e-04,\n",
      "          9.6057e-03, -4.6522e-03, -3.1837e-03,  2.4074e-04, -2.6279e-03,\n",
      "          1.5111e-03, -6.7385e-04,  2.2003e-05, -3.1299e-03,  4.7390e-04,\n",
      "         -1.4171e-03, -6.8985e-05, -1.2882e-03, -8.8297e-03,  4.9910e-03,\n",
      "         -1.5463e-03,  4.9672e-03]], grad_fn=<SelectBackward0>), 'blocks.2.attn.b_V': tensor([[ 0.0230, -0.0350,  0.0233, -0.1914,  0.1431, -0.0992, -0.1002,  0.0397,\n",
      "          0.1683,  0.0372, -0.0207, -0.0747,  0.1688,  0.0545, -0.1420, -0.0231,\n",
      "         -0.0840, -0.0023,  0.0263, -0.0149,  0.0183,  0.0146,  0.0092, -0.0249,\n",
      "          0.0207,  0.0963,  0.0164,  0.1547, -0.0094, -0.0263,  0.0203, -0.0073],\n",
      "        [ 0.0180,  0.1331, -0.0022, -0.1103,  0.0054,  0.0203, -0.0110,  0.0967,\n",
      "          0.1545, -0.0128, -0.1278, -0.0142,  0.0240, -0.0002,  0.1405, -0.0009,\n",
      "         -0.0198,  0.1367,  0.0392,  0.0126,  0.0233,  0.0387, -0.1369, -0.0419,\n",
      "          0.0436,  0.0728,  0.0021,  0.0094, -0.1082,  0.1322,  0.0079, -0.0122]],\n",
      "       grad_fn=<SelectBackward0>), 'blocks.2.attn.W_O': tensor([[[-1.9475e-02,  2.1745e-02,  7.0628e-03,  ..., -1.5018e-01,\n",
      "           5.0600e-02,  3.4042e-01],\n",
      "         [ 7.7021e-02, -1.4837e-02, -1.5726e-03,  ...,  1.5994e-01,\n",
      "          -8.0096e-02, -2.2232e-01],\n",
      "         [ 1.6314e-01,  1.4428e-01, -1.1371e-03,  ...,  1.0108e+00,\n",
      "           5.8035e-01, -7.2081e-01],\n",
      "         ...,\n",
      "         [-1.2510e-01, -2.0696e-01,  1.8706e-02,  ..., -9.9416e-01,\n",
      "          -5.5910e-01,  6.2378e-01],\n",
      "         [-2.0576e-02,  1.1616e-01,  1.2752e-02,  ...,  4.8372e-03,\n",
      "           2.3870e-02,  2.1153e-01],\n",
      "         [ 4.6688e-02,  5.4349e-02, -1.0846e-02,  ...,  8.3179e-02,\n",
      "          -4.6363e-02, -6.9201e-03]],\n",
      "\n",
      "        [[ 9.7210e-02, -1.5615e-01, -6.4624e-02,  ...,  2.2950e-01,\n",
      "           4.1223e-02,  1.2683e+00],\n",
      "         [-2.9204e-01, -3.2317e-02, -6.6098e-04,  ...,  1.2903e-01,\n",
      "          -3.7784e-01, -7.2879e-01],\n",
      "         [ 5.3900e-02,  1.7897e-01,  1.1250e-02,  ..., -4.8700e-01,\n",
      "          -1.0472e-01, -7.1482e-01],\n",
      "         ...,\n",
      "         [-3.8276e-01,  5.0764e-02,  1.5919e-02,  ..., -6.5949e-02,\n",
      "          -4.7616e-01, -3.8776e-01],\n",
      "         [-4.7219e-02, -3.1996e-02,  2.1760e-02,  ..., -1.8513e-02,\n",
      "          -1.2324e-01,  2.5553e-01],\n",
      "         [-9.2643e-03,  8.4456e-02,  2.1758e-02,  ..., -6.6600e-01,\n",
      "          -1.6205e-02, -8.0415e-01]]], grad_fn=<ReshapeAliasBackward0>), 'blocks.2.attn.b_O': Parameter containing:\n",
      "tensor([ 2.3454e-01,  5.3126e-02,  3.1701e-03,  4.8591e-03,  7.0782e-03,\n",
      "         7.2922e-01,  5.0486e-02,  2.2741e-02, -1.4350e-01, -5.1027e-01,\n",
      "         4.3159e-01,  2.9813e-02,  1.7583e-01,  1.8272e-02, -3.6234e-02,\n",
      "         4.0315e-03, -6.4136e-01,  8.5184e-04,  1.5228e-02, -6.9552e-01,\n",
      "        -1.5679e-02,  6.3552e-02, -6.7018e-04,  1.8646e-03,  2.3352e-03,\n",
      "         4.6972e-01,  1.2339e-01, -1.7700e-01,  1.1748e-01, -2.8493e-01,\n",
      "         1.1004e-02,  1.8951e-03, -3.4984e-01,  4.8657e-03, -1.8863e-01,\n",
      "         3.8241e-03,  2.7532e-02,  1.5968e-03, -7.0503e-01,  1.9623e-02,\n",
      "         2.9124e-03,  2.8426e-04,  2.5628e-02,  1.4327e-01,  4.1109e-01,\n",
      "        -1.2650e-01, -1.0551e-01,  3.3474e-01, -4.3115e-02,  4.1473e-01,\n",
      "         5.4102e-02, -3.3974e-01,  1.3005e-02, -2.5640e-02,  7.0849e-02,\n",
      "         4.4674e-03,  8.1018e-03,  7.1454e-01,  4.0119e-03,  1.0842e-01,\n",
      "         1.7046e-04, -1.4222e-01,  4.8935e-02,  7.2690e-02],\n",
      "       requires_grad=True), 'blocks.2.ln2.w': Parameter containing:\n",
      "tensor([ 6.5642e-01,  1.0154e+00,  3.2054e-03,  1.0868e-01,  5.5072e-01,\n",
      "         4.2108e-01,  6.8518e-01,  9.0894e-01,  5.9753e-01,  4.0980e-01,\n",
      "         6.2779e-01,  2.3016e-01,  7.6334e-01,  6.5553e-01,  1.0530e+00,\n",
      "         5.3764e-01,  5.1820e-01,  3.6906e-03, -9.7698e-03,  2.2803e-01,\n",
      "         9.0700e-01,  9.0626e-01,  7.9885e-01,  4.7010e-03, -3.3420e-03,\n",
      "         7.5294e-01,  8.9514e-03,  1.1062e+00,  1.1959e+00,  6.9448e-01,\n",
      "         1.3087e+00,  1.0134e-01,  5.0439e-01, -5.9875e-03,  7.8662e-01,\n",
      "         7.6199e-04,  2.8580e-01, -5.7823e-03,  3.6883e-01,  8.0216e-01,\n",
      "         3.5639e-01,  4.7803e-03,  4.0593e-03, -1.2879e-03,  6.5281e-01,\n",
      "         8.0760e-01,  1.3814e+00,  5.1344e-01,  7.5555e-01,  1.0859e+00,\n",
      "         9.3722e-01,  9.0482e-01,  1.2319e+00,  1.6145e-04,  2.0890e-03,\n",
      "         6.3189e-01,  4.2365e-02,  4.3620e-01,  2.0102e-03,  4.6809e-01,\n",
      "         3.7850e-03,  1.0745e+00,  8.6787e-01, -1.4984e-03],\n",
      "       requires_grad=True), 'blocks.2.ln2.b': Parameter containing:\n",
      "tensor([-3.2500e-01,  9.5665e-02,  6.4188e-04, -1.6676e-02,  2.5270e-02,\n",
      "        -4.7836e-01, -9.2012e-02,  7.5816e-02,  7.1283e-02,  4.3857e-01,\n",
      "        -4.1518e-01, -1.8914e-03,  7.7684e-03,  6.3199e-03,  9.5175e-02,\n",
      "        -2.3100e-02,  4.0338e-01, -2.6377e-04, -6.1855e-04,  5.6356e-01,\n",
      "        -1.0123e-02, -1.1017e-02,  3.3508e-02, -1.2865e-03, -1.9612e-04,\n",
      "        -2.8865e-01, -3.0871e-03, -7.8438e-03, -8.3853e-02,  2.8603e-01,\n",
      "        -6.7876e-02,  4.9321e-03,  3.7655e-01, -1.6056e-03, -1.5871e-02,\n",
      "         1.3730e-03, -1.0746e-02, -6.5696e-04,  5.3365e-01, -7.7692e-02,\n",
      "         3.6106e-02, -6.1023e-05, -1.2440e-03, -2.1075e-03, -3.2845e-01,\n",
      "         4.6575e-02, -1.2444e-01, -3.8907e-01,  1.2802e-01,  1.5297e-04,\n",
      "        -1.6502e-01,  2.6351e-01,  4.4938e-02,  1.0813e-03, -1.4818e-03,\n",
      "        -6.0822e-02, -1.1142e-03, -5.2040e-01,  1.8742e-03, -1.4762e-01,\n",
      "         2.8316e-03, -3.6555e-02,  5.0790e-03,  9.5867e-05],\n",
      "       requires_grad=True), 'blocks.2.mlp.W_in': Parameter containing:\n",
      "tensor([[-0.3315,  0.0613,  0.0773,  ..., -0.0485,  0.1161,  0.0858],\n",
      "        [ 0.1935,  0.0149,  0.0287,  ..., -0.0162,  0.2667,  0.1151],\n",
      "        [ 0.0170, -0.0139, -0.0207,  ...,  0.0144, -0.0088,  0.0053],\n",
      "        ...,\n",
      "        [-0.2183, -0.0498, -0.0271,  ...,  0.0147, -0.0676,  0.0669],\n",
      "        [-0.2453, -0.0430, -0.0224,  ..., -0.0106, -0.0430,  0.0588],\n",
      "        [ 0.0057, -0.0275,  0.0307,  ...,  0.0012,  0.0491,  0.0363]],\n",
      "       requires_grad=True), 'blocks.2.mlp.b_in': Parameter containing:\n",
      "tensor([-7.7526e-02, -2.6746e-01, -1.8680e-02, -3.0952e-01,  1.6678e-02,\n",
      "        -2.1584e-01, -1.2673e-02, -2.7792e-01, -2.5085e-01, -6.9416e-03,\n",
      "         2.0656e-04, -2.8487e-01,  6.4442e-03, -3.0183e-01,  9.3847e-04,\n",
      "        -6.5395e-03, -1.3205e-02, -1.4521e-01, -1.4248e-02, -3.1335e-01,\n",
      "        -2.3882e-01, -4.4304e-03, -2.3011e-02, -2.3828e-02, -3.1444e-03,\n",
      "        -2.2468e-01, -7.8303e-03,  2.5513e-02, -1.9194e-01, -3.4007e-04,\n",
      "         5.3781e-03, -1.6647e-02, -9.6697e-03, -1.0878e-02,  3.0329e-03,\n",
      "        -1.8545e-01, -4.2521e-01,  1.0117e-02,  2.6240e-03, -3.3989e-01,\n",
      "        -2.9381e-01,  1.3682e-02,  5.7018e-02, -5.7267e-03, -2.8514e-02,\n",
      "        -2.9981e-01, -2.6311e-01, -1.6653e-01, -3.3107e-01,  1.8617e-02,\n",
      "        -1.5818e-02, -3.0113e-01,  1.0194e-02, -2.4954e-01, -1.0610e-01,\n",
      "        -3.1728e-01, -6.6864e-03, -1.7873e-02, -2.5031e-01, -1.6706e-02,\n",
      "        -5.3672e-03, -4.9630e-03,  9.4207e-03, -3.3885e-01, -3.4544e-01,\n",
      "        -1.1298e-03, -1.1686e-03,  4.9792e-03, -2.2735e-01, -1.5139e-02,\n",
      "        -1.2109e-02, -1.0041e-03, -3.7729e-01, -3.5476e-02, -3.3751e-01,\n",
      "         3.1453e-03, -1.5571e-02, -1.7342e-01,  3.3504e-04, -3.1996e-01,\n",
      "        -2.0987e-02, -1.2782e-01,  1.0083e-02, -9.5305e-03, -9.9558e-03,\n",
      "        -9.1316e-02, -1.5832e-02, -4.2658e-01, -2.8103e-01, -8.6526e-03,\n",
      "        -1.9603e-01,  4.6570e-03,  8.2650e-03, -3.3603e-01, -2.2174e-02,\n",
      "        -1.3068e-01, -9.0543e-03, -3.8682e-01, -3.6240e-02,  6.6716e-03,\n",
      "         1.1379e-02, -2.8699e-02, -2.3181e-01, -2.7964e-01, -2.2307e-01,\n",
      "        -3.1951e-01, -2.0893e-02, -1.9944e-01, -4.5853e-01, -3.9053e-01,\n",
      "        -3.2643e-01, -3.0952e-01, -1.2808e-01, -3.3486e-03, -2.8819e-01,\n",
      "        -1.9055e-02, -1.5971e-03, -8.3121e-03,  5.2935e-03, -3.2214e-03,\n",
      "         1.9428e-02,  1.2433e-02,  8.0851e-03,  9.6931e-04,  5.7760e-03,\n",
      "        -3.4758e-01,  1.1051e-02, -2.3721e-01, -1.8496e-02, -3.3826e-01,\n",
      "        -8.1679e-04,  1.9091e-02, -4.5426e-03, -3.3911e-01, -2.5652e-01,\n",
      "        -3.4137e-01,  4.0804e-03, -4.5240e-01, -1.9535e-02, -1.9225e-02,\n",
      "         9.9707e-03, -6.0707e-02, -2.4886e-03,  4.8310e-03, -2.4728e-01,\n",
      "        -1.8154e-01, -1.5061e-02, -1.1308e-02,  5.4044e-03,  7.0301e-03,\n",
      "        -2.6816e-01,  2.5366e-03,  1.5759e-02, -2.9442e-01, -2.2876e-01,\n",
      "         9.2084e-03, -1.2735e-02,  1.2113e-03, -3.5838e-01,  2.3649e-02,\n",
      "        -1.5858e-02, -1.3367e-03, -5.8871e-03, -1.4128e-02, -4.6367e-03,\n",
      "        -2.4233e-01,  2.8157e-02, -2.7765e-01, -7.0660e-04, -3.0159e-02,\n",
      "        -3.3738e-01, -1.5889e-01, -1.0102e-02,  6.9967e-03, -1.7825e-02,\n",
      "        -3.1840e-01, -1.9137e-04, -9.4856e-03, -5.3367e-01, -5.6523e-03,\n",
      "         3.3041e-03, -3.7101e-02, -2.7924e-01, -1.0276e-02, -1.4570e-03,\n",
      "        -3.5268e-01, -1.8238e-01, -2.8264e-01,  4.6683e-03,  1.2537e-02,\n",
      "        -3.0794e-01, -1.2315e-02, -4.4127e-03, -1.6964e-02, -1.5257e-02,\n",
      "        -2.9192e-01, -3.4562e-01, -1.4545e-01, -9.4598e-03, -4.1567e-03,\n",
      "         2.7338e-02,  1.5710e-02,  3.0440e-03, -3.0628e-01,  1.1634e-02,\n",
      "        -1.8077e-02,  5.2006e-03, -5.5176e-03, -3.1887e-01, -2.9058e-01,\n",
      "        -4.4580e-04, -6.6323e-03, -1.2956e-02, -7.5758e-03, -6.7690e-03,\n",
      "        -7.2127e-03,  4.8866e-03, -3.8976e-01, -4.5197e-01, -2.8968e-01,\n",
      "        -3.3209e-01, -2.3058e-01, -1.5203e-02, -1.6388e-03, -1.2817e-02,\n",
      "        -4.3645e-01,  3.2437e-03, -1.2850e-02, -5.8611e-02, -2.5373e-01,\n",
      "        -3.2585e-01, -1.2479e-01, -2.2764e-01,  9.4567e-03, -1.6530e-02,\n",
      "        -8.2591e-03,  1.1191e-02,  5.9391e-03, -3.1341e-03, -2.8133e-02,\n",
      "        -9.5174e-03, -2.1004e-01, -2.8608e-01, -3.1594e-01, -2.2683e-01,\n",
      "        -2.7812e-01, -1.5000e-01, -1.5831e-02, -1.8903e-01,  1.1092e-02,\n",
      "        -2.3260e-01, -5.9765e-03,  2.5241e-02,  2.3816e-03, -1.7729e-01,\n",
      "        -3.1209e-01], requires_grad=True), 'blocks.2.mlp.W_out': Parameter containing:\n",
      "tensor([[-0.2322,  0.0843,  0.0755,  ...,  0.3444, -0.1136,  0.0179],\n",
      "        [-0.2022, -0.0916,  0.0201,  ...,  0.2116,  0.0837, -0.0144],\n",
      "        [ 0.0206, -0.2135,  0.0042,  ..., -0.0927, -0.1571,  0.1957],\n",
      "        ...,\n",
      "        [ 0.0327, -0.1796, -0.0210,  ...,  0.0422,  0.0584, -0.0125],\n",
      "        [-0.1149,  0.1311, -0.0839,  ...,  0.1645, -0.0144,  0.4482],\n",
      "        [-0.1828, -0.0728,  0.0117,  ...,  0.1902,  0.0846, -0.0090]],\n",
      "       requires_grad=True), 'blocks.2.mlp.b_out': Parameter containing:\n",
      "tensor([ 0.2435,  0.0805,  0.0034, -0.0008,  0.0620,  0.6706, -0.0305,  0.2058,\n",
      "        -0.0948, -0.4763,  0.5803,  0.0186,  0.0938,  0.1617,  0.2023,  0.0014,\n",
      "        -0.5872,  0.0019,  0.0060, -0.6690,  0.0165, -0.1866,  0.0106,  0.0039,\n",
      "         0.0010,  0.5392,  0.0495, -0.4018, -0.1772, -0.3073, -0.0080, -0.0024,\n",
      "        -0.3395,  0.0011, -0.2796,  0.0036, -0.0119, -0.0301, -0.6514, -0.0159,\n",
      "         0.0009, -0.0012, -0.0021,  0.0543,  0.5917, -0.0539,  0.2992,  0.2937,\n",
      "         0.1431,  0.5028, -0.0622, -0.4863,  0.2982, -0.2160, -0.0405, -0.1206,\n",
      "         0.0032,  0.6536,  0.0018,  0.0996,  0.0034, -0.1841, -0.0702,  0.0131],\n",
      "       requires_grad=True), 'ln_final.w': Parameter containing:\n",
      "tensor([ 6.4799e-03, -6.7147e-03,  1.4038e+00,  1.4580e+00, -3.6690e-02,\n",
      "         1.3827e-01, -1.7412e-04,  2.8574e-03,  1.0906e-03,  2.9124e-01,\n",
      "         2.6219e-01,  9.2166e-03, -5.1929e-04, -6.4697e-03, -5.8945e-03,\n",
      "         8.9659e-03,  1.7478e-01,  1.4271e+00,  3.3593e-01,  1.3403e-01,\n",
      "        -2.4965e-02,  1.5588e-02,  1.3246e+00,  1.1275e+00,  1.3645e+00,\n",
      "         2.0971e-01,  2.2412e-03,  4.0624e-03, -3.6600e-03,  3.7523e-01,\n",
      "         4.1229e-04,  1.2262e+00,  4.1282e-01,  9.7888e-01,  2.6955e-05,\n",
      "         1.4388e+00,  4.3549e-04, -1.6427e-02,  1.4676e-01,  9.4815e-04,\n",
      "         1.0825e+00,  1.1104e+00,  5.6215e-03, -1.6415e-03,  1.5505e-01,\n",
      "        -5.7029e-03,  3.6156e-04,  3.1787e-01, -1.2919e-02,  2.3073e-03,\n",
      "        -3.1915e-02,  2.6292e-04, -5.8764e-04,  1.8956e-02, -1.3363e-02,\n",
      "         4.7877e-04,  1.2435e+00,  1.4146e-01,  1.5806e+00, -3.2435e-04,\n",
      "         1.3693e+00,  2.8752e-03, -8.0328e-03,  2.7373e-02],\n",
      "       requires_grad=True), 'ln_final.b': Parameter containing:\n",
      "tensor([-4.6347e-03,  2.9219e-04,  7.1028e-03, -1.1876e-02,  1.4970e-03,\n",
      "        -4.8023e-01,  8.9818e-06, -4.1637e-04,  5.0838e-04,  4.1890e-01,\n",
      "        -3.0824e-01, -4.8198e-04, -9.9435e-05,  6.2587e-04,  9.9233e-04,\n",
      "        -9.8005e-05,  4.4570e-01,  5.7473e-03, -6.6694e-03,  3.7083e-01,\n",
      "         3.6833e-04,  1.6887e-03, -9.5677e-03,  7.9603e-04,  2.8595e-03,\n",
      "        -1.9736e-01,  1.0858e-04,  2.2228e-03, -7.5490e-04,  2.7411e-01,\n",
      "        -2.7612e-05, -1.7782e-02,  3.9707e-01, -1.0048e-02, -3.1812e-04,\n",
      "        -1.2292e-03,  1.1515e-04,  1.8950e-04,  5.1903e-01, -2.6603e-04,\n",
      "        -7.8666e-03, -5.5474e-03,  1.8123e-04,  6.6677e-05, -1.5575e-01,\n",
      "        -1.0098e-03, -1.7078e-05, -3.2451e-01,  9.1861e-04, -1.7141e-03,\n",
      "         5.1027e-03,  2.6243e-04,  2.6707e-04,  1.3930e-03,  4.5847e-04,\n",
      "         2.6186e-05, -6.9407e-04, -4.6351e-01,  4.8722e-04,  1.9219e-04,\n",
      "         6.7339e-03,  6.0265e-04,  4.2102e-04, -1.6321e-05],\n",
      "       requires_grad=True)}\n"
     ]
    }
   ],
   "source": [
    "def format_flat_weights(model):\n",
    "    formatted_flat_weights = {}\n",
    "\n",
    "    formatted_flat_weights[\"embed.W_E\"] = model._backbone.wte.weight\n",
    "    formatted_flat_weights[\"pos_embed.W_pos\"] = model._backbone.wpe.weight\n",
    "\n",
    "    for l in range(args.model.n_layer):\n",
    "        formatted_flat_weights[f\"blocks.{l}.ln1.w\"] = model._backbone.h[l].ln_1.weight\n",
    "        formatted_flat_weights[f\"blocks.{l}.ln1.b\"] = model._backbone.h[l].ln_1.bias\n",
    "\n",
    "        # In GPT-2, q,k,v are produced by one big linear map, whose output is\n",
    "        # concat([q, k, v])\n",
    "        W = model._backbone.h[l].attn.c_attn.weight\n",
    "        W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)\n",
    "        W_Q = einops.rearrange(W_Q, \"m (i h)->i m h\", i=args.model.n_head)\n",
    "        W_K = einops.rearrange(W_K, \"m (i h)->i m h\", i=args.model.n_head)\n",
    "        W_V = einops.rearrange(W_V, \"m (i h)->i m h\", i=args.model.n_head)\n",
    "\n",
    "        formatted_flat_weights[f\"blocks.{l}.attn.W_Q\"] = W_Q\n",
    "        formatted_flat_weights[f\"blocks.{l}.attn.W_K\"] = W_K\n",
    "        formatted_flat_weights[f\"blocks.{l}.attn.W_V\"] = W_V\n",
    "\n",
    "        qkv_bias = model._backbone.h[l].attn.c_attn.bias\n",
    "        qkv_bias = einops.rearrange(\n",
    "            qkv_bias,\n",
    "            \"(qkv index head)->qkv index head\",\n",
    "            qkv=3,\n",
    "            index=args.model.n_head,\n",
    "            head=32, # TODO: What should this number be?\n",
    "        )\n",
    "        formatted_flat_weights[f\"blocks.{l}.attn.b_Q\"] = qkv_bias[0]\n",
    "        formatted_flat_weights[f\"blocks.{l}.attn.b_K\"] = qkv_bias[1]\n",
    "        formatted_flat_weights[f\"blocks.{l}.attn.b_V\"] = qkv_bias[2]\n",
    "\n",
    "        W_O = model._backbone.h[l].attn.c_proj.weight\n",
    "        W_O = einops.rearrange(W_O, \"(i h) m->i h m\", i=args.model.n_head)\n",
    "        formatted_flat_weights[f\"blocks.{l}.attn.W_O\"] = W_O\n",
    "        formatted_flat_weights[f\"blocks.{l}.attn.b_O\"] = model._backbone.h[l].attn.c_proj.bias\n",
    "\n",
    "        formatted_flat_weights[f\"blocks.{l}.ln2.w\"] = model._backbone.h[l].ln_2.weight\n",
    "        formatted_flat_weights[f\"blocks.{l}.ln2.b\"] = model._backbone.h[l].ln_2.bias\n",
    "\n",
    "        W_in = model._backbone.h[l].mlp.c_fc.weight\n",
    "        formatted_flat_weights[f\"blocks.{l}.mlp.W_in\"] = W_in\n",
    "        formatted_flat_weights[f\"blocks.{l}.mlp.b_in\"] = model._backbone.h[l].mlp.c_fc.bias\n",
    "\n",
    "        W_out = model._backbone.h[l].mlp.c_proj.weight\n",
    "        formatted_flat_weights[f\"blocks.{l}.mlp.W_out\"] = W_out\n",
    "        formatted_flat_weights[f\"blocks.{l}.mlp.b_out\"] = model._backbone.h[l].mlp.c_proj.bias\n",
    "    # formatted_flat_weights[\"unembed.W_U\"] = gpt2.lm_head.weight.T # ?\n",
    "\n",
    "    formatted_flat_weights[\"ln_final.w\"] = model._backbone.ln_f.weight\n",
    "    formatted_flat_weights[\"ln_final.b\"] = model._backbone.ln_f.bias\n",
    "\n",
    "    return formatted_flat_weights\n",
    "\n",
    "flat_weights = format_flat_weights(model)\n",
    "print(flat_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_read_in', '_backbone.wpe', '_backbone.drop', '_backbone.h.0.ln_1', '_backbone.h.0.attn.c_attn', '_backbone.h.0.attn.attn_dropout', '_backbone.h.0.attn.c_proj', '_backbone.h.0.attn.resid_dropout', '_backbone.h.0.attn', '_backbone.h.0.ln_2', '_backbone.h.0.mlp.c_fc', '_backbone.h.0.mlp.act', '_backbone.h.0.mlp.c_proj', '_backbone.h.0.mlp.dropout', '_backbone.h.0.mlp', '_backbone.h.0', '_backbone.h.1.ln_1', '_backbone.h.1.attn.c_attn', '_backbone.h.1.attn.attn_dropout', '_backbone.h.1.attn.c_proj', '_backbone.h.1.attn.resid_dropout', '_backbone.h.1.attn', '_backbone.h.1.ln_2', '_backbone.h.1.mlp.c_fc', '_backbone.h.1.mlp.act', '_backbone.h.1.mlp.c_proj', '_backbone.h.1.mlp.dropout', '_backbone.h.1.mlp', '_backbone.h.1', '_backbone.h.2.ln_1', '_backbone.h.2.attn.c_attn', '_backbone.h.2.attn.attn_dropout', '_backbone.h.2.attn.c_proj', '_backbone.h.2.attn.resid_dropout', '_backbone.h.2.attn', '_backbone.h.2.ln_2', '_backbone.h.2.mlp.c_fc', '_backbone.h.2.mlp.act', '_backbone.h.2.mlp.c_proj', '_backbone.h.2.mlp.dropout', '_backbone.h.2.mlp', '_backbone.h.2', '_backbone.ln_f', '_backbone', '_read_out', ''])\n"
     ]
    }
   ],
   "source": [
    "def rec_update_dict(keys, val, dict):\n",
    "    if len(keys) == 1:\n",
    "        dict[keys[0]][\"val\"] = val\n",
    "        return dict\n",
    "    if keys[0] not in dict:\n",
    "        dict[keys[0]] = rec_update_dict(keys[1:], val, {})\n",
    "        return dict\n",
    "    dict[keys[0]] = rec_update_dict(keys[1:], val, dict[keys[0]])\n",
    "    return dict\n",
    "\n",
    "def get_hier_activations(flat_activations):\n",
    "    hier_activations = {}\n",
    "\n",
    "    for key, val in flat_activations.items():\n",
    "        hier_activations = rec_update_dict(key.split(\".\"), val, hier_activations)\n",
    "\n",
    "    return hier_activations\n",
    "\n",
    "def get_flat_activations(model, X):\n",
    "    flat_activations = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            flat_activations[name] = output\n",
    "        return hook\n",
    "    \n",
    "    def hook_model(model):\n",
    "        for name, module in model.named_modules():\n",
    "            module.register_forward_hook(get_activation(name))\n",
    "\n",
    "    hook_model(model)\n",
    "    model(X)\n",
    "\n",
    "    return flat_activations\n",
    "\n",
    "flat_activations = get_flat_activations(model, X)\n",
    "# hier_activations = get_hier_activations(flat_activations)\n",
    "print(flat_activations.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability\n",
    "\n",
    "See line 996 of [`TransformerLens/transformer_lens/loading_from_pretrained.py`](https://github.com/neelnanda-io/TransformerLens/blob/main/transformer_lens/loading_from_pretrained.py#L996)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 192])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([2, 21, 192])\n",
      "torch.Size([2, 21, 64])\n",
      "torch.Size([2, 21, 64])\n"
     ]
    }
   ],
   "source": [
    "# def get_attention(xs, ys, model, samples=3):\n",
    "#     \"\"\"Returns an attention matrix of full sequence inputs evaluated at a small number of samples\"\"\"\n",
    "#     pred = model(xs[:samples], ys[:samples], output_attentions=True)  # Run model\n",
    "#     attention = pred[-1]  # Retrieve attention from model outputs of a model evaluated at \n",
    "#     tokens = [val for pair in zip([f\"x{i}\" for i in range(xs[0].shape[0])], [f\"y{i}={ys[0][i].item():.1f}\" for i in range(xs[0].shape[0])]) for val in pair]\n",
    "#     return attention, tokens\n",
    "\n",
    "def get_attention_matrix(flat_activations):\n",
    "    # TODO: Write\n",
    "    print(flat_activations[\"_backbone.h.1.attn.c_attn\"].shape)\n",
    "    print(flat_activations[\"_backbone.h.1.attn.c_proj\"].shape)\n",
    "    print(flat_activations[\"_backbone.h.0.attn.resid_dropout\"].shape)\n",
    "\n",
    "get_attention_matrix(flat_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(get_attention(X, y, model))\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mget_attention\u001b[0;34m(xs, ys, model, samples)\u001b[0m\n\u001b[1;32m      3\u001b[0m pred \u001b[39m=\u001b[39m model(xs[:samples], ys[:samples], output_attentions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Run model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m attention \u001b[39m=\u001b[39m pred[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# Retrieve attention from model outputs of a model evaluated at \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokens \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(xs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])], [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mys[\u001b[39m0\u001b[39m][i]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(xs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]) \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m pair]\n\u001b[1;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m attention, tokens\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m pred \u001b[39m=\u001b[39m model(xs[:samples], ys[:samples], output_attentions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Run model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m attention \u001b[39m=\u001b[39m pred[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# Retrieve attention from model outputs of a model evaluated at \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokens \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(xs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])], [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mys[\u001b[39m0\u001b[39m][i]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(xs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]) \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m pair]\n\u001b[1;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m attention, tokens\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "print(get_attention(X, y, model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attn_matrix(attn_matrix layer, head, sample, task):\n",
    "    # print(x.shape)\n",
    "\n",
    "    attention, _ = get_attention(x, y, model)\n",
    "    attn_mat = attention[layer].detach().cpu()[sample][head]\n",
    "\n",
    "    gs = gridspec.GridSpec(3, 4, width_ratios=[0.25, 0.25, 1, 1],\n",
    "    wspace=0.04, hspace=0.04, top=0.95, bottom=0.05, left=0.17, right=0.845) \n",
    "\n",
    "    attn_x = attn_mat[::2]\n",
    "    attn_y = attn_mat[1::2]\n",
    "\n",
    "    k_vals = range(2, 10)\n",
    "    scores_x = []\n",
    "    scores_y = []\n",
    "\n",
    "    for k in k_vals:\n",
    "        kmeans_x = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(attn_x)\n",
    "        scores_x.append(silhouette_score(attn_x, kmeans_x.labels_))\n",
    "        kmeans_y = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(attn_y)\n",
    "        scores_y.append(silhouette_score(attn_y, kmeans_y.labels_))\n",
    "\n",
    "    plt.plot(k_vals, scores_x, label=\"scores x\")\n",
    "    plt.plot(k_vals, scores_y, label=\"scores y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    k_x = k_vals[np.argmax(np.asarray(scores_x))]\n",
    "    k_y = k_vals[np.argmax(np.asarray(scores_y))]\n",
    "\n",
    "    kmeans_x = KMeans(n_clusters=k_x, random_state=0, n_init=\"auto\").fit(attn_x)\n",
    "    kmeans_y = KMeans(n_clusters=k_y, random_state=0, n_init=\"auto\").fit(attn_y)\n",
    "\n",
    "    kmeans_all = [kmeans_x, kmeans_y]\n",
    "\n",
    "    for i, (kmeans, k) in enumerate(zip([kmeans_x, kmeans_y], [k_x, k_y])):\n",
    "        if i == 0:\n",
    "            print(\"x query cluster:\")\n",
    "        else:\n",
    "            print(\"y query cluster:\")\n",
    "\n",
    "        cls = svm.LinearSVC(multi_class=\"ovr\").fit(x.cpu()[i], kmeans.labels_)\n",
    "        if k == 2:\n",
    "            print(f\"SVM coefs: {cls.coef_[0]}\")\n",
    "            print(f\"class1: {np.where(kmeans.labels_ == 0)[0]}\")\n",
    "            print_dominant_literals(kmeans, x, sample, 0)\n",
    "            # print_cluster_y_weight(kmeans, ys, i, 0)\n",
    "            print_cluster_pathways(kmeans, x, y, model, task, sample, 0)\n",
    "            print(f\"class2: {np.where(kmeans.labels_ == 1)[0]}\")\n",
    "            print_dominant_literals(kmeans, x, sample, 1)\n",
    "            # print_cluster_y_weight(kmeans, ys, i, 1)\n",
    "            print_cluster_pathways(kmeans, x, y, model, task, sample, 1)\n",
    "        else:\n",
    "            for c in range(k):\n",
    "                c_labels = np.where(kmeans.labels_ == c)[0]\n",
    "                print(f\"SVM coefs for class {c}: {cls.coef_[c]}\\\\class elements: {c_labels}\")\n",
    "                print_dominant_literals(kmeans, x, sample, c)\n",
    "                # print_cluster_y_weight(kmeans, ys, i, c)\n",
    "                print_cluster_pathways(kmeans, x, y, model, task, sample, c)\n",
    "\n",
    "\n",
    "    # print(kmeans_x.labels_)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "    # ax5 = plt.subplot(3, 4, 7)\n",
    "    ax5 = plt.subplot(gs[1,2])\n",
    "    plt.imshow(attn_mat[::2, ::2], vmin=0, vmax=1, cmap='Purples')\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax5.get_yticklabels(), visible=False)\n",
    "\n",
    "    # ax6 = plt.subplot(3, 4, 8, sharey=ax5)\n",
    "    ax6 = plt.subplot(gs[1,3])\n",
    "    plt.imshow(attn_mat[::2, 1::2], vmin=0, vmax=1, cmap='Purples')\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax6.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax6.get_xticklabels(), visible=False)\n",
    "\n",
    "    # ax8 = plt.subplot(3, 4, 11, sharex=ax5)\n",
    "    ax8 = plt.subplot(gs[2,2])\n",
    "    plt.imshow(attn_mat[1::2, ::2], vmin=0, vmax=1, cmap='Purples')\n",
    "    plt.grid(None)\n",
    "    plt.xlabel(\"x key\")\n",
    "    plt.setp(ax8.get_yticklabels(), visible=False)\n",
    "\n",
    "    # ax9 = plt.subplot(3, 4, 12, sharey=ax8, sharex=ax6)\n",
    "    ax9 = plt.subplot(gs[2,3])\n",
    "    plt.imshow(attn_mat[1::2, 1::2], vmin=0, vmax=1, cmap='Purples')\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax9.get_yticklabels(), visible=False)\n",
    "    plt.xlabel(\"y key\")\n",
    "\n",
    "    # ax45 = plt.subplot(3, 4, 6, sharey=ax5)\n",
    "    ax45 = plt.subplot(gs[1,1])\n",
    "    plt.imshow(np.tile(kmeans_x.labels_, (10, 1)).T)\n",
    "    plt.setp(ax45.get_xticklabels(), visible=False)\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax45.get_yticklabels(), visible=False)\n",
    "\n",
    "    # print(np.tile(kmeans_x.labels_, (10, 1)).T)\n",
    "\n",
    "    # ax78 = plt.subplot(3, 4, 10, sharey=ax7, sharex=ax45)\n",
    "    ax78 = plt.subplot(gs[2,1])\n",
    "    plt.imshow(np.tile(kmeans_y.labels_, (10, 1)).T)\n",
    "    plt.setp(ax78.get_xticklabels(), visible=False)\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax78.get_yticklabels(), visible=False)\n",
    "\n",
    "    # ax4 = plt.subplot(3, 4, 5, sharey=ax45)\n",
    "    ax4 = plt.subplot(gs[1,0])\n",
    "    plt.imshow(x.cpu()[sample] >= 0)\n",
    "    plt.ylabel(\"x query\")\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "\n",
    "    # ax7 = plt.subplot(3, 4, 9, sharex=ax4)\n",
    "    ax7 = plt.subplot(gs[2,0])\n",
    "    plt.imshow(y.cpu()[None, sample].tile(20,1).T, cmap=\"bwr\")\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax7.get_xticklabels(), visible=False)\n",
    "    plt.ylabel(\"y query\")\n",
    "\n",
    "    # ax2 = plt.subplot(3, 4, 3, sharex=ax5)\n",
    "    ax2 = plt.subplot(gs[0,2])\n",
    "    plt.imshow(x.cpu()[sample].transpose(1,0) >= 0)\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "\n",
    "    # ax3 = plt.subplot(3, 4, 4, sharex=ax6, sharey=ax2)\n",
    "    ax3 = plt.subplot(gs[0,3])\n",
    "    plt.imshow(y.cpu()[None, sample].tile(20,1), cmap=\"bwr\")\n",
    "    plt.grid(None)\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax3.get_yticklabels(), visible=False)\n",
    "\n",
    "    # plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    \n",
    "    fig.suptitle(f\"layer {layer}, head {head}, round {sample}\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "analyze_attn_matrix(xs_boolean, ys_boolean, model, layer, head, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
