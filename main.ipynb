{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Around with Model Selection Small Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from data_gen import get_data\n",
    "from models import TransformerModel\n",
    "import wandb\n",
    "import yaml\n",
    "from munch import Munch\n",
    "\n",
    "with open(f\"configs/model_selection.yaml\", \"r\") as yaml_file:\n",
    "    args = Munch.fromYAML(yaml_file)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data(alphas=args.data.data_alphas, N=args.data.N, d_d=args.data.d_d, train_samp_per_class=args.data.train_samp_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(dataloader, model, loss_fn, optimizer, verbose=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "        if verbose:\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return avg_loss/size\n",
    "\n",
    "def nn_test(dataloader, model, loss_fn, verbose=False):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "        \n",
    "        test_loss /= num_batches\n",
    "        if verbose:\n",
    "            print(f\"Test Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(model, xs, ys, optimizer, loss_func):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(xs, ys)\n",
    "#     loss = loss_func(output, ys)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.detach().item(), output.detach()\n",
    "\n",
    "# def sample_seeds(total_seeds, count):\n",
    "#     seeds = set()\n",
    "#     while len(seeds) < count:\n",
    "#         seeds.add(randint(0, total_seeds - 1))\n",
    "#     return seeds\n",
    "\n",
    "def train(model, X_train, y_train, X_test, y_test):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.training.lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.training.batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.training.batch_size)\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    for t in range(args.training.epochs):\n",
    "        new_train_loss = nn_train(train_dataloader, model, loss_fn, optimizer)\n",
    "        new_test_loss = nn_test(test_dataloader, model, loss_fn)\n",
    "        train_loss.append(new_train_loss)\n",
    "        test_loss.append(new_test_loss)\n",
    "\n",
    "        if t % 10 == 0:\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            print(f\"Train Loss: {new_train_loss}, Test Loss: {new_test_loss}\")\n",
    "        \n",
    "        if t % args.training.save_every_epochs == 0:\n",
    "            pass\n",
    "        \n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train_loss\": new_train_loss,\n",
    "                \"test_loss\": new_test_loss\n",
    "            },\n",
    "            step=t,\n",
    "        )\n",
    "\n",
    "    return train_loss, test_loss\n",
    "\n",
    "\n",
    "    # for epoch in epochs:\n",
    "    #     loss, output = train_step(model, X.to(device), y.to(device), optimizer, loss_func)\n",
    "\n",
    "    #     point_wise_tags = list(range(curriculum.n_points))\n",
    "    #     point_wise_loss_func = task.get_metric()\n",
    "    #     point_wise_loss = point_wise_loss_func(output, ys.to(device)).mean(dim=0)\n",
    "    #     if model.n_out > 1:\n",
    "    #         point_wise_loss = point_wise_loss.mean(dim=-1)\n",
    "\n",
    "    #     baseline_loss = (sum(max(curriculum.n_dims_truncated - ii, 0) for ii in range(curriculum.n_points)) / curriculum.n_points)\n",
    "\n",
    "    #     if i % args.wandb.log_every_steps == 0 and not args.test_run:\n",
    "    #         wandb.log(\n",
    "    #             {\n",
    "    #                 \"overall_loss\": loss,\n",
    "    #                 \"excess_loss\": loss / baseline_loss,\n",
    "    #                 \"pointwise/loss\": dict(\n",
    "    #                     zip(point_wise_tags, point_wise_loss.cpu().numpy())\n",
    "    #                 ),\n",
    "    #                 \"n_points\": curriculum.n_points,\n",
    "    #                 \"n_dims\": curriculum.n_dims_truncated,\n",
    "    #                 \"n_embd\": args.model.n_embd\n",
    "    #             },\n",
    "    #             step=i,\n",
    "    #         )\n",
    "\n",
    "    #     curriculum.update()\n",
    "\n",
    "    #     pbar.set_description(f\"loss {loss}\")\n",
    "    #     if i % args.training.save_every_steps == 0 and not args.test_run:\n",
    "    #         training_state = {\n",
    "    #             \"model_state_dict\": model.state_dict(),\n",
    "    #             \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    #             \"train_step\": i,\n",
    "    #         }\n",
    "    #         torch.save(training_state, state_path)\n",
    "\n",
    "    #     if (\n",
    "    #         args.training.keep_every_steps > 0\n",
    "    #         and i % args.training.keep_every_steps == 0\n",
    "    #         and not args.test_run\n",
    "    #         and i > 0\n",
    "    #     ):\n",
    "    #         torch.save(model.state_dict(), os.path.join(args.out_dir, f\"model_{i}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data_dict):\n",
    "    alphas_merged = torch.cat([torch.ones(args.data.train_samp_per_class)*a for a in data_dict], dim=0)\n",
    "    X_merged = torch.cat([torch.cat([torch.unsqueeze(data_dict[a][b][\"y_hat\"], 2) for b in data_dict[a]] + [torch.unsqueeze(data_dict[a][a][\"y_test\"], 2)], dim=2) for a in data_dict], dim=0)\n",
    "    X_merged[:, -1, -1] = 0\n",
    "    y_merged = torch.cat([data_dict[a][a][\"y_test\"][:, -1] for a in data_dict], dim=0)\n",
    "\n",
    "    randperm = torch.randperm(alphas_merged.shape[0])\n",
    "\n",
    "    alphas = alphas_merged[randperm]\n",
    "    X = X_merged[randperm]\n",
    "    y = y_merged[randperm]\n",
    "\n",
    "    print(f\"Alphas: {alphas.shape}, X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "    return alphas, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphas: torch.Size([20000]), X: torch.Size([20000, 21, 3]), y: torch.Size([20000])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:sjp4udlr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5620eae5a3a4b30b0ef882596f44918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model_selection</strong> at: <a href='https://wandb.ai/berkott/modelSelectInterp/runs/sjp4udlr' target=\"_blank\">https://wandb.ai/berkott/modelSelectInterp/runs/sjp4udlr</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>models/wandb/run-20230614_114842-sjp4udlr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:sjp4udlr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb81bb8498d4b2ca097308962ac0d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670415732854358, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>models/wandb/run-20230614_114932-bvyczem4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/berkott/modelSelectInterp/runs/bvyczem4' target=\"_blank\">model_selection</a></strong> to <a href='https://wandb.ai/berkott/modelSelectInterp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/berkott/modelSelectInterp' target=\"_blank\">https://wandb.ai/berkott/modelSelectInterp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/berkott/modelSelectInterp/runs/bvyczem4' target=\"_blank\">https://wandb.ai/berkott/modelSelectInterp/runs/bvyczem4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Loss: 0.02077634597523138, Test Loss: 0.4719072085618973\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train Loss: 0.012692446633242071, Test Loss: 0.41670012509822846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[39m=\u001b[39m TransformerModel(\n\u001b[1;32m     11\u001b[0m     n_dims\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(args\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdata_alphas) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     12\u001b[0m     n_positions\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     n_embd\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mn_embd\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m wandb\u001b[39m.\u001b[39minit(\u001b[39mdir\u001b[39m\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mout_dir,\n\u001b[1;32m     19\u001b[0m     project\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mwandb\u001b[39m.\u001b[39mproject,\n\u001b[1;32m     20\u001b[0m     entity\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mwandb\u001b[39m.\u001b[39mentity,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     name\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mwandb\u001b[39m.\u001b[39mname,\n\u001b[1;32m     24\u001b[0m     resume\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m train_loss, test_loss \u001b[39m=\u001b[39m train(model, X_train, y_train, X_test, y_test)\n",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     26\u001b[0m test_loss \u001b[39m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mtraining\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m---> 29\u001b[0m     new_train_loss \u001b[39m=\u001b[39m nn_train(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m     30\u001b[0m     new_test_loss \u001b[39m=\u001b[39m nn_test(test_dataloader, model, loss_fn)\n\u001b[1;32m     31\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(new_train_loss)\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mnn_train\u001b[0;34m(dataloader, model, loss_fn, optimizer, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alphas, X, y = format_data(data_dict)\n",
    "\n",
    "alphas_train = alphas[:int(alphas.shape[0]*args.data.train_prop)]\n",
    "alphas_test = alphas[int(alphas.shape[0]*args.data.train_prop):]\n",
    "X_train = X[:int(X.shape[0]*args.data.train_prop)]\n",
    "X_test = X[int(X.shape[0]*args.data.train_prop):]\n",
    "y_train = y[:int(y.shape[0]*args.data.train_prop)]\n",
    "y_test = y[int(y.shape[0]*args.data.train_prop):]\n",
    "\n",
    "model = TransformerModel(\n",
    "    n_dims=len(args.data.data_alphas) + 1,\n",
    "    n_positions=args.data.N,\n",
    "    n_layer=args.model.n_layer,\n",
    "    n_head=args.model.n_head,\n",
    "    n_embd=args.model.n_embd\n",
    ")\n",
    "\n",
    "wandb.init(dir=args.out_dir,\n",
    "    project=args.wandb.project,\n",
    "    entity=args.wandb.entity,\n",
    "    config=args.__dict__,\n",
    "    notes=args.wandb.notes,\n",
    "    name=args.wandb.name,\n",
    "    resume=True\n",
    ")\n",
    "\n",
    "train_loss, test_loss = train(model, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
