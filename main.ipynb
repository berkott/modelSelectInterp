{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Around with Model Selection Small Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data_gen import get_data\n",
    "from models import TransformerModel\n",
    "\n",
    "N=21\n",
    "d_d=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(N=21, d_d=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, xs, ys, optimizer, loss_func):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(xs, ys)\n",
    "    loss = loss_func(output, ys)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.detach().item(), output.detach()\n",
    "\n",
    "def sample_seeds(total_seeds, count):\n",
    "    seeds = set()\n",
    "    while len(seeds) < count:\n",
    "        seeds.add(randint(0, total_seeds - 1))\n",
    "    return seeds\n",
    "\n",
    "def train(model):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    starting_step = 0\n",
    "    # state_path = os.path.join(args.out_dir, \"state.pt\")\n",
    "    # if os.path.exists(state_path):\n",
    "    #     state = torch.load(state_path)\n",
    "    #     model.load_state_dict(state[\"model_state_dict\"])\n",
    "    #     optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "    #     starting_step = state[\"train_step\"]\n",
    "    #     for i in range(state[\"train_step\"] + 1):\n",
    "    #         curriculum.update()\n",
    "\n",
    "    # n_dims = model.n_dims\n",
    "    bsize = 32\n",
    "    num_training_examples = 20000\n",
    "\n",
    "    # task = task_sampler(**task_sampler_args)\n",
    "    xs = data_sampler.sample_xs(curriculum.n_points, bsize, curriculum.n_dims_truncated, **data_sampler_args)\n",
    "    ys = task.evaluate(xs)\n",
    "\n",
    "    # Use MSE loss\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    loss, output = train_step(model, xs.to(device), ys.to(device), optimizer, loss_func)\n",
    "\n",
    "    point_wise_tags = list(range(curriculum.n_points))\n",
    "    point_wise_loss_func = task.get_metric()\n",
    "    point_wise_loss = point_wise_loss_func(output, ys.to(device)).mean(dim=0)\n",
    "    if model.n_out > 1:\n",
    "        point_wise_loss = point_wise_loss.mean(dim=-1)\n",
    "\n",
    "    baseline_loss = (sum(max(curriculum.n_dims_truncated - ii, 0) for ii in range(curriculum.n_points)) / curriculum.n_points)\n",
    "\n",
    "    if i % args.wandb.log_every_steps == 0 and not args.test_run:\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"overall_loss\": loss,\n",
    "                \"excess_loss\": loss / baseline_loss,\n",
    "                \"pointwise/loss\": dict(\n",
    "                    zip(point_wise_tags, point_wise_loss.cpu().numpy())\n",
    "                ),\n",
    "                \"n_points\": curriculum.n_points,\n",
    "                \"n_dims\": curriculum.n_dims_truncated,\n",
    "                \"n_embd\": args.model.n_embd\n",
    "            },\n",
    "            step=i,\n",
    "        )\n",
    "\n",
    "    curriculum.update()\n",
    "\n",
    "    pbar.set_description(f\"loss {loss}\")\n",
    "    if i % args.training.save_every_steps == 0 and not args.test_run:\n",
    "        training_state = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"train_step\": i,\n",
    "        }\n",
    "        torch.save(training_state, state_path)\n",
    "\n",
    "    if (\n",
    "        args.training.keep_every_steps > 0\n",
    "        and i % args.training.keep_every_steps == 0\n",
    "        and not args.test_run\n",
    "        and i > 0\n",
    "    ):\n",
    "        torch.save(model.state_dict(), os.path.join(args.out_dir, f\"model_{i}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    n_dims=d_d,\n",
    "    n_positions=N\n",
    ")\n",
    "\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
